{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b44fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mysql.connector import connect\n",
    "from elasticsearch import  helpers\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'D:\\markding_git\\big-data-ai-integration-platform\\airflow\\dags\\code')\n",
    "import es_mapping\n",
    "from connection import ElasticSearchConnectionManager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56168c3b",
   "metadata": {},
   "source": [
    "# Alphabet generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the string module\n",
    "import string\n",
    "alphabet_list = list(string.ascii_lowercase)\n",
    "\n",
    "print(alphabet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bcc69",
   "metadata": {},
   "source": [
    "# ES connection settup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ef3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_es_connection():\n",
    "#     # Load certificate path and credentials from environment variables\n",
    "#     cafile_path = os.getenv('ES_CAFILE_PATH', r\"D:\\markding_git\\big-data-ai-integration-platform\\search_system\\es_certs\\http_ca.crt\")\n",
    "#     es_host = os.getenv('ES_HOST', 'http://localhost:9200')\n",
    "#     es_user = os.getenv('ES_USER', 'elastic')\n",
    "#     es_password = os.getenv('ES_PASSWORD', 'gAcstb8v-lFCVzCBC__a')\n",
    "#     # uWzW*VOzBZknR-Jt3T7i\n",
    "#     context = create_default_context(cafile=cafile_path)\n",
    "#     # context.check_hostname = True  # Ensure hostname is checked\n",
    "#     # context.verify_mode = CERT_REQUIRED  # Ensure the certificate is verified\n",
    "\n",
    "#     context.check_hostname = False\n",
    "#     context.verify_mode = CERT_NONE\n",
    "    \n",
    "#     es = Elasticsearch(\n",
    "#                 [es_host],\n",
    "#                 http_auth=(es_user, es_password),\n",
    "#                 verify_certs=False,\n",
    "#                 )\n",
    "#     return es\n",
    "\n",
    "# es = create_es_connection()\n",
    "es = ElasticSearchConnectionManager.get_instance()\n",
    "\n",
    "print(es.info())\n",
    "\n",
    "es_health = es.health_report()\n",
    "print(es_health['status'])\n",
    "indicators = es_health['indicators']\n",
    "[f\"status_of_[{i}] : {indicators[i]['status']}\" for i in indicators.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_mapping(df, column_types_dict):\n",
    "    \"\"\"\n",
    "    Generate Elasticsearch mapping based on the DataFrame and a column-to-Python type dictionary.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to be indexed.\n",
    "        column_types_dict (dict): Dictionary mapping columns to Python types.\n",
    "\n",
    "    Returns:\n",
    "        dict: Elasticsearch mapping dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a mapping from Python types to Elasticsearch types\n",
    "    type_conversion = {\n",
    "        'object': 'keyword',  # Typically strings in Pandas\n",
    "        'int64': 'integer',\n",
    "        'int32': 'integer',\n",
    "        'float64': 'float',\n",
    "        'float32': 'float',\n",
    "        'bool': 'boolean',\n",
    "        'datetime64[ns]': 'date',\n",
    "        'timedelta[ns]': 'long',\n",
    "        'category': 'keyword',\n",
    "    }\n",
    "\n",
    "    # Generate the Elasticsearch mapping using the provided column types\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                col: {\n",
    "                    \"type\": type_conversion.get(column_types_dict[col], \"text\"),  # Default to \"text\"\n",
    "                    \"ignore_malformed\": True if type_conversion.get(column_types_dict[col]) in [\"integer\", \"float\"] else False\n",
    "                }\n",
    "                for col in df.columns\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8757945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a MySQL connection\n",
    "def mysql_connection_whole_corp():\n",
    "    return connect(host='localhost',\n",
    "                   port=3307,\n",
    "                   user='root',\n",
    "                   password='!QAZ2wsx',\n",
    "                   database='whole_corp')\n",
    "\n",
    "con = mysql_connection_whole_corp()\n",
    "cursor = con.cursor()\n",
    "\n",
    "# cursor.execute('select * from whole_corp limit 100')\n",
    "# result = cursor.fetchall()\n",
    "# df = pd.DataFrame(result, columns= cursor.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146afd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2759468"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute('select * from wholecorp_clusters_vector')\n",
    "df_vector = pd.DataFrame(cursor.fetchall(), columns=cursor.column_names)\n",
    "\n",
    "# df_vector.drop_duplicates(subset='統一編號',inplace=True)\n",
    "\n",
    "len(df_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2087dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379734"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector.drop_duplicates(subset='統一編號',inplace=True)\n",
    "\n",
    "len(df_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96107f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('select * from wholecorp_clusters_vector_1')\n",
    "df_vector_1 = pd.DataFrame(cursor.fetchall(), columns=cursor.column_names)\n",
    "\n",
    "df_vector_1.drop_duplicates(subset='統一編號',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd063b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_vector_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4907e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib, os, csv\n",
    "\n",
    "user = \"root\"\n",
    "password = \"!QAZ2wsx\"\n",
    "host = \"localhost\"\n",
    "port = 3307\n",
    "db   = \"whole_corp\"\n",
    "tbl  = \"wholecorp_clusters_vector_1\"\n",
    "\n",
    "encoded_pw = urllib.parse.quote_plus(password)\n",
    "# IMPORTANT: allow_local_infile=1 for mysqlconnector (or local_infile=1 for PyMySQL)\n",
    "db_url = f\"mysql+mysqlconnector://{user}:{encoded_pw}@{host}:{port}/{db}?allow_local_infile=1\"\n",
    "\n",
    "engine = create_engine(db_url, pool_pre_ping=True)\n",
    "\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(text(\"\"\"\n",
    "#         CREATE TABLE IF NOT EXISTS whole_corp (\n",
    "#             統一編號 CHAR(8) NOT NULL,\n",
    "#             公司名稱 VARCHAR(50),\n",
    "#             負責人 VARCHAR(20),\n",
    "#             登記地址 VARCHAR(200),\n",
    "#             資本額 VARCHAR(50),\n",
    "#             營業項目及代碼表 VARCHAR(200),\n",
    "#             縣市名稱 VARCHAR(20),\n",
    "#             區域名稱 VARCHAR(20),\n",
    "#             縣市區域 VARCHAR(20),\n",
    "#             類別_全 VARCHAR(200),\n",
    "#             官網 VARCHAR(100),\n",
    "#             電話 VARCHAR(20),\n",
    "#             PRIMARY KEY (統一編號)\n",
    "#         ) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\n",
    "#     \"\"\"))\n",
    "# 1) Ensure table exists with the right schema.\n",
    "#    If you don't have it yet, create it once with to_sql but with zero rows\n",
    "#    (or write a CREATE TABLE manually for precise types).\n",
    "#    Example (create empty table once):\n",
    "df_vector.head(0).to_sql(tbl, engine, if_exists=\"append\", index=False)\n",
    "\n",
    "# 2) Write a clean TSV to disk (much safer than CSV for commas/quotes)\n",
    "tmp_path = \"/tmp/whole_corp_load.tsv\"  # on Windows: r\"C:\\temp\\whole_corp_load.tsv\"\n",
    "os.makedirs(os.path.dirname(tmp_path), exist_ok=True)\n",
    "\n",
    "# Write TSV: represent NULLs as \\N (MySQL understands \\N as NULL if not quoted)\n",
    "df_vector.to_csv(\n",
    "    tmp_path,\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    na_rep=\"\\\\N\",\n",
    "    quoting=csv.QUOTE_NONE,\n",
    "    escapechar=\"\\\\\",\n",
    ")\n",
    "\n",
    "# 3) Bulk load\n",
    "with engine.begin() as conn:\n",
    "\n",
    "    # optional but helps: disable checks while loading\n",
    "    conn.execute(text(\"SET FOREIGN_KEY_CHECKS=0\"))\n",
    "    conn.execute(text(\"SET UNIQUE_CHECKS=0\"))\n",
    "    # If you have heavy indexes on the table, consider dropping them before and recreating after.\n",
    "\n",
    "    # LOAD DATA (LOCAL needs allow_local_infile=1)\n",
    "    conn.execute(text(f\"\"\"\n",
    "        LOAD DATA LOCAL INFILE :path\n",
    "        INTO TABLE {tbl}\n",
    "        FIELDS TERMINATED BY '\\t'\n",
    "        ESCAPED BY '\\\\\\\\'\n",
    "        LINES TERMINATED BY '\\n'\n",
    "        IGNORE 1 LINES\n",
    "    \"\"\"), {\"path\": tmp_path})\n",
    "\n",
    "    conn.execute(text(\"SET UNIQUE_CHECKS=1\"))\n",
    "    conn.execute(text(\"SET FOREIGN_KEY_CHECKS=1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861c379",
   "metadata": {},
   "source": [
    "# Mapping for each tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff47777",
   "metadata": {},
   "source": [
    "# Create index with mapping in ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de891e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    es = create_es_connection()\n",
    "    index_name = 'whole_corp'\n",
    "    if es.indices.exists(index = index_name):\n",
    "        if str(input('Index {index_name} already exists. Delete? [y/n]').lower()) == 'y':\n",
    "            es.indices.delete(index=index_name)\n",
    "        else:\n",
    "            print('count:', es.count(index = index_name))\n",
    "\n",
    "    response = es.indices.create(index=index_name, body=corp_mapping())\n",
    "    \n",
    "    # Checking mapping in each index\n",
    "    print(f\"/n=============response, es.indices.get_mapping(index={index_name})=============\")\n",
    "    print(response, es.indices.get_mapping(index=index_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86915691",
   "metadata": {},
   "source": [
    "# Adjusted mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696148ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------Create mapping for user----------------------------------'''\n",
    "def corp_mapping():\n",
    "    # Define the settings for the custom analyzer\n",
    "    body = {\n",
    "        \"properties\": {\n",
    "          \"cluster\": {\n",
    "            \"type\": \"keyword\"\n",
    "          },\n",
    "          \"vector\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 100    # // must match your model output\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "\n",
    "    return body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.put_mapping(index='whole_corp', body=corp_mapping())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43512027",
   "metadata": {},
   "source": [
    "# Get index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.get_mapping(index='whole_corp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380fc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"query\":{\n",
    "        \"bool\":{\n",
    "        \"must\":{\n",
    "            'exists': { \"field\": \"vector\"}\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.search(index='whole_corp', body=body, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f3ed74",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86121530",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = create_es_connection()\n",
    "for index_name in ['whole_corp']:\n",
    "    try:\n",
    "        print(f'index name : [{index_name}]',es.count(index=index_name.lower()))\n",
    "    except Exception as e:\n",
    "        print(f'\\n==============no values in index name [{index_name}]================')\n",
    "es.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete(index='whole_corp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da7268",
   "metadata": {},
   "source": [
    "# Data import to ES from mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd94e18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''=============================================['userESG', 'lifeCircleESG', 'courseESG']==============================================='''\n",
    "# Function to create a MySQL connection\n",
    "def mysql_connection_whole_corp():\n",
    "    return connect(host='localhost',\n",
    "                   port=3307,\n",
    "                   user='root',\n",
    "                   password='!QAZ2wsx',\n",
    "                   database='whole_corp')\n",
    "\n",
    "\n",
    "def json_serial(obj):\n",
    "    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n",
    "    if isinstance(obj, datetime.datetime):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, bytearray):\n",
    "        return obj.decode('utf-8')\n",
    "    raise TypeError (\"Type %s not serializable\" % type(obj))\n",
    "    \n",
    "\n",
    "# Generator function to fetch data in batches from MySQL\n",
    "def fetch_data(cursor, batch_size=1000):\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        # Convert each row to JSON serializable format\n",
    "        cleaned_rows = []\n",
    "        for row in rows:\n",
    "            cleaned_row = {k: json_serial(v) if isinstance(v, (datetime.datetime, bytearray)) else 0.0 if (k == '資本額') and (v == '\\\\N') else v for k, v in row.items()}\n",
    "            cleaned_rows.append(cleaned_row)\n",
    "        yield cleaned_rows\n",
    "    \n",
    "\n",
    "def elastic_import(es, actions):\n",
    "    response, errors  = helpers.bulk(es, actions, raise_on_error=False)\n",
    "    if errors:\n",
    "        print(f'==================================errors: ===============================\\n{errors}')\n",
    "        \n",
    "    \n",
    "# Main function to control the flow of the script\n",
    "def main(table_name, database = None):\n",
    "    # Connect to MySQL\n",
    "    \n",
    "    mysql_conn = mysql_connection_whole_corp() \n",
    "        \n",
    "    cursor = mysql_conn.cursor(dictionary=True)\n",
    "    \n",
    "    # Define the SQL query and execute it\n",
    "    sql_query = f\"SELECT * FROM {table_name}\"\n",
    "    cursor.execute(sql_query)\n",
    "    \n",
    "    # Connect to Elasticsearch\n",
    "    es = create_es_connection()\n",
    "\n",
    "    # Define the Elasticsearch index where the data will be stored\n",
    "    es_index = table_name.lower()\n",
    "    \n",
    "    # Fetch data from MySQL and prepare it for Elasticsearch\n",
    "#     if es.indices.exists(index = table_name):\n",
    "#         return 'the index already exists'\n",
    "    \n",
    "    for batch in fetch_data(cursor):\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": es_index,\n",
    "                \"_source\": row,\n",
    "            } for row in batch\n",
    "        ]\n",
    "        \n",
    "        # Bulk index the data in Elasticsearch\n",
    "        elastic_import(es, actions)\n",
    "\n",
    "    # Cleanup\n",
    "    cursor.close()\n",
    "    mysql_conn.close()\n",
    "    return actions\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Define paramenter\n",
    "    database = 'whole_corp'\n",
    "    table_name = 'whole_corp'\n",
    "    es_data = main(table_name,database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89195940",
   "metadata": {},
   "source": [
    "# Update index with specific field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, boto3\n",
    "import es_mapping\n",
    "from connection import ElasticSearchConnectionManager\n",
    "import sys\n",
    "\n",
    "sys.path.append(r'C:\\Users\\mark.ding\\big-data-ai-integration-platform\\common')\n",
    "from logger import initlog\n",
    "\n",
    "logger = initlog(__name__)\n",
    "\n",
    "# JSON serializer for datetime and bytearray objects\n",
    "def json_serial(obj):\n",
    "    if isinstance(obj, datetime.datetime):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, bytearray):\n",
    "        return obj.decode('utf-8')\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Fetch data from MySQL in batches\n",
    "def fetch_data(cursor, batch_size=1000):\n",
    "    while True:\n",
    "        rows = cursor.fetchmany(batch_size)\n",
    "        if not rows:\n",
    "            break\n",
    "        cleaned_rows = [\n",
    "            {k: json_serial(v) if isinstance(v, (datetime.datetime, bytearray)) else v for k, v in row.items()}\n",
    "            for row in rows\n",
    "        ]\n",
    "        logger.debug(f\"Fetched {len(cleaned_rows)} rows from MySQL.\")\n",
    "        yield cleaned_rows\n",
    "\n",
    "# Search and update document in Elasticsearch\n",
    "def search_and_update_field(es, index, source_id, vector_val, cluster_val):\n",
    "    try:\n",
    "        search_query = {\"query\": {\"term\": {\"統一編號\": source_id}}}\n",
    "        search_response = es.search(index=index, body=search_query)\n",
    "        hits = search_response['hits']['hits']\n",
    "        \n",
    "        if not hits:\n",
    "            logger.info(f\"Document with source ID {source_id} does not exist.\")\n",
    "            return False\n",
    "\n",
    "        for hit in hits:\n",
    "            doc_id = hit['_id']\n",
    "            response = es.update(index=index, id=doc_id, body={\"doc\": {\n",
    "                \"vector\": vector_val,\n",
    "                \"cluster\" :cluster_val\n",
    "            }})\n",
    "            logger.debug(f\"Document with source ID {source_id} updated: {response}\")\n",
    "            return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating document with source ID {source_id}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Bulk import data to Elasticsearch\n",
    "def elastic_import(es, actions):\n",
    "    try:\n",
    "        response, errors = helpers.bulk(es, actions, raise_on_error=False)\n",
    "        logger.info(f\"Imported {response} documents to Elasticsearch, errors: {errors}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Bulk import errors: {e}\")\n",
    "\n",
    "# Update data in Elasticsearch\n",
    "def update_data_to_es(es, cursor, es_index, table_name):\n",
    "    updated_count = 0\n",
    "    create_data_count = 0\n",
    "    actions = []\n",
    "\n",
    "    for batch in fetch_data(cursor):\n",
    "        logger.debug(f\"Processing batch with {len(batch)} records.\")\n",
    "        \n",
    "        df = pd.DataFrame(batch)\n",
    "        df = df.where(pd.notnull(df), None)  # Replace NaN with None\n",
    "        df = df.replace({np.nan: None})  # Replace NaN with None\n",
    "\n",
    "        batch = df.to_dict(orient='records')\n",
    "\n",
    "        for row in batch:\n",
    "            source_id = row['統一編號']\n",
    "            # logger.info(f\"Updating document with source ID {source_id}...\")\n",
    "            try:\n",
    "                print('row:========', row)\n",
    "                doc_exists = search_and_update_field(es, es_index, source_id, row.get('vector'), row.get('cluster'))\n",
    "                if not doc_exists:\n",
    "                    actions.append({\"_index\": es_index, \"_source\": row})\n",
    "                    create_data_count += 1\n",
    "                    logger.debug(f\"Document with source ID {source_id} created.\")\n",
    "                else:\n",
    "                    updated_count += 1\n",
    "                    logger.debug(f\"Document with source ID {source_id} updated.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error updating document with source ID {source_id}: {e}\")\n",
    "\n",
    "        if actions:\n",
    "            logger.info(f\"Importing {len(actions)} documents to Elasticsearch index: [{es_index}]...]\")\n",
    "            elastic_import(es, actions)\n",
    "            actions = []  # Clear actions after bulk import\n",
    "\n",
    "    logger.info(f\"[{table_name}] total documents updated: {updated_count}, and created: {create_data_count}\")\n",
    "    return updated_count, create_data_count\n",
    "\n",
    "# Main function to handle the ETL process\n",
    "def etl_process(table_name, es, es_index):\n",
    "    logger.info(f\"Starting ETL process for table: {table_name}.\")\n",
    "    \n",
    "    # Create MySQL connection\n",
    "    mysql_conn = ElasticSearchConnectionManager.mysql_connection_whole_corp() \n",
    "    cursor = mysql_conn.cursor(dictionary=True)\n",
    "\n",
    "    date_now_python = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    date_now_sql_query = \"SELECT now()\"\n",
    "    cursor.execute(date_now_sql_query)\n",
    "    date_now_sql = cursor.fetchone()\n",
    "    date_now_sql = date_now_sql['now()'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Check if date time is the same in Python and SQL\n",
    "    if date_now_python != date_now_sql:\n",
    "        logger.info(\"Date time is not the same: %s and %s\", date_now_python, date_now_sql)\n",
    "        cursor.execute(\"SET time_zone = 'Asia/Taipei';\")\n",
    "    else:\n",
    "        logger.info(\"Date time is the same: %s\", date_now_python)\n",
    "\n",
    "    # Query to fetch updated records\n",
    "    sql_query_updatedAt = f\"SELECT * FROM {table_name}\"\n",
    "    cursor.execute(sql_query_updatedAt)\n",
    "\n",
    "    # Update data in Elasticsearch\n",
    "    update_data_count, create_data_count = update_data_to_es(es, cursor, es_index, table_name)\n",
    "\n",
    "    mysql_conn.close()\n",
    "    logger.info(f\"ETL process for table: {table_name} completed with {update_data_count} updates and {create_data_count} creations.\")\n",
    "    return update_data_count, create_data_count\n",
    "\n",
    "\n",
    "def create_index_if_not_exists(es, index_name):\n",
    "    try:\n",
    "        es.indices.get_mapping(index=index_name)\n",
    "        logger.info(f\"Index {index_name} already exists.\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Index {index_name} does not exist. Creating new index.\")\n",
    "        mapping = es_mapping.corp_mapping()\n",
    "        if mapping:\n",
    "            es.indices.create(index=index_name, body=mapping)\n",
    "            logger.info(f\"Index {index_name} created with the specified mapping.\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to create index {index_name}. No valid mapping found.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    logger.info(\"ETL process started.\")\n",
    "    es = ElasticSearchConnectionManager.get_instance()\n",
    "    \n",
    "    table_name = 'wholecorp_clusters_vector'\n",
    "    \n",
    "    index_name = table_name.lower()\n",
    "    \n",
    "    create_index_if_not_exists(es, index_name)\n",
    "    \n",
    "    update_data_count, create_data_count = etl_process(table_name, es, index_name)\n",
    "    \n",
    "    logger.info(f\"Data update for {table_name} completed, with update_data_count: {update_data_count}, create_data_count: {create_data_count}\")\n",
    "    \n",
    "    logger.info(\"ETL process finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ETL process failed due to: {str(e)}\", exc_info=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexva_penta_integration_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
