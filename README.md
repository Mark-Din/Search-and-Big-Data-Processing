# arXiv Data Lakehouse & Search System

## ğŸ§© Project Summary
A complete end-to-end data pipeline built with **Spark**, **Kafka**, **Delta Lake**, **MinIO**, and **Elasticsearch**.  
It ingests and cleans arXiv metadata, performs clustering and vector embedding, and provides a **Streamlit interface** for semantic search, recommendations, and pipeline monitoring.

---

## ğŸ—ï¸ System Architecture Overview
This project implements an end-to-end data pipeline for arXiv and some other companies' metadata processing, analytics, and search.

### ğŸ—ºï¸ High-Level Data Flow
The following diagram illustrates the overall flow of data from ingestion to the user interface.

![High-Level Architecture](projects/arXiv_lakehouse/architecture_overview.png)

### âš™ï¸ Logical Process Flow
The diagram below details how each component interacts and how data moves through the system.

![Logical Workflow](projects/arXiv_lakehouse/architecture_flowchart.png)

**Layer Summary**
- Ingestion â†’ ETL â†’ Feature Engineering â†’ Clustering â†’ Elasticsearch â†’ Streamlit + FastAPI  
- Bronze (MySQL) â†’ Silver (Delta cleaned) â†’ Gold (feature vectors) â†’ Elasticsearch (vector index)

---

## ğŸ§  System Components
| Stage | Description |
|--------|--------------|
| **Ingestion** | Python scripts ingest data from the arXiv OAI/API and store it in MySQL (Bronze). |
| **Cleaning** | `task_silver_clean.py` â€” Spark ETL cleans, deduplicates, and joins data â†’ Delta (Silver). |
| **Feature Engineering** | `task_build_features_gold.py` â€” TF-IDF feature generation â†’ Delta (Gold). |
| **Clustering** | `task_data_clustering.py` â€” SVD + KMeans clustering â†’ write vectors to Elasticsearch. |
| **Storage** | Delta Lake on MinIO for Silver/Gold layers. |
| **Search API** | FastAPI integrates with Elasticsearch for vector and keyword search. |
| **Frontend** | Streamlit UI for search, recommendations, and visualization. |
| **Monitoring** | MySQL + `ETL_metrics.py` for pipeline logs, status, and performance. |
| **Environment** | Dockerized (Airflow, Spark, MySQL, MinIO, Elasticsearch, Streamlit). |

---

## ğŸ”„ ETL Flow Description
| Stage | Script | Description |
|--------|---------|-------------|
| **Bronze** | `etl_data_to_mysql_OAI.py`& `etl_data_to_mysql_api.py`| Pull raw XML â†’ store in MySQL. |
| **Silver** | `task_silver_clean.py` | Clean, deduplicate, and join â†’ Delta Lake. |
| **Gold** | `task_build_features_gold.py` | TF-IDF vectorization â†’ Delta Lake. |
| **Clustering** | `task_data_clustering.py` | Dimensionality reduction (SVD) + KMeans â†’ Elasticsearch. |
| **Monitoring** | `ETL_metrics.py` | Track ETL runs and visualize pipeline metadata. |
| **Analysis** | `etl_coauthorship_edges.py` | Basic visualizaion on data |
---

## ğŸ“Š Streamlit Dashboards

### 1. Search UI (`home.py`)
- Query arXiv data via FastAPI endpoints.  
- View results, clusters, and recommended papers.

### 2. Monitoring Dashboard (`ETL_metrics.py`)
- Latest run summary (records, duration, success rate).  
- Trend over time and stage breakdown.  
- Status bar charts and logs.

### 3. Academic Trends Dashboard (`arxiv_dashboard.py`)
- Average updates per discipline.  
- Median time to publication.  
- Cumulative submissions per author/institution.  
- Co-authorship network visualization.

![alt text](projects/arXiv_lakehouse/monitoring_dashboard.png)
<!-- - Co-authorship network visualization. -->

---

## ğŸ“ˆ Key Metrics
- **Pipeline metrics:** total records processed, duration, success rate.  
- **Data metrics:** unique papers, categories, authors.  
- **Search metrics:** total hits, latency, cluster coverage.

---

## âš™ï¸ Tech Stack
| Category | Tools |
|-----------|-------|
| Programming | Python 3.x |
| Orchestration | Apache Airflow |
| Data Processing | PySpark, Delta Lake |
| Database | MySQL |
| Object Storage | MinIO |
| Search Engine | Elasticsearch |
| Frontend | Streamlit + FastAPI |
| Visualization | Altair |
| Environment | Docker & Docker Compose |

> **Note:** `hadoop-aws-3.3.2.jar` is required specifically for Airflow to interact with MinIO/S3.

---

## ğŸš€ How to Run

1. **Clone the repository**
   ```bash
   git clone https://github.com/Mark-Din/arXiv_data_processing
   cd airflow
2. Start Airflow and PostgreSQL
    ```bash
    # You need to have Astronomer.Astro installed first
    astro dev start
    ```
3. Start other services
    ```bash
    docker-compose up --build
    ``` 
4. Access UIs
- MinIO: http://localhost:9001
- Spark: http://localhost:8080
- Streamlit: http://localhost:8501

## â˜ï¸ Cloud Adaptability
| Local Component | AWS Equivalent |
|-----------|-------|
| MinIO | Amazon S3 |
| Spark | AWS Glue / EMR |
| Airflow | MWAA |
| Elasticsearch | OpenSearch |
| Streamlit | QuickSight / CloudWatch Dashboards|
| MySQL | DynamoDB / RDS |

## ğŸ™ Acknowledgments
### Datasets
- arXiv.org Open Access Metadata
- Kaggle arXiv Dataset

## ğŸ§® Institutional Rankings (Future Integration)
The architecture is designed to integrate external datasets such as journal impact factors, CORE conference rankings, and citation databases.
These enrichments can be ingested into the Bronze layer and joined with arXiv metadata in Silver/Gold layers to produce subject-wise institutional rankings.
This version focuses on core ETL, feature extraction, and semantic search, but is structured for future expansion.

## ğŸ“¢ Further Note
The architecture was implemented fully on-premise using Docker and MinIO to ensure a self-contained, reproducible setup within the limited project timeline.
Each component mirrors a cloud equivalent (e.g., MinIO â†” S3, Spark â†” AWS Glue), allowing future migration to AWS with minimal refactoring.

## ğŸ§­ Assumptions
- Each arXiv `paper_id` uniquely identifies a single paper across versions.
- Category mapping (`category_map`) is stable across all records.
- The pipeline runs daily and overwrites Silver/Gold layers for simplicity.
- On-premise setup (MinIO, Docker) simulates cloud-based architecture for demonstration.

## ğŸ§© Challenges & Irregularities
- **Inconsistent date formats:** The `updated` and `created` fields varied across OAI records, requiring normalization to standard ISO date strings.  
- **Missing abstracts or titles:** Some records lacked abstracts, which were dropped to maintain clean text input for TF-IDF.  
- **Duplicate records:** Multiple versions of the same paper required deduplication in the Silver layer using the latest `version_created`.  
- **Nested XML parsing:** OAI responses contained irregular XML tags that needed defensive parsing logic.
