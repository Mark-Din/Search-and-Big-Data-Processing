# # ---------------
# # search_system
# # ---------------
services:
  # For custome purpose
  mysql_db:
    build:
      context: ./sql
      dockerfile: Dockerfile
    container_name: mysql_db_container
    environment:
      MYSQL_ROOT_PASSWORD: "!QAZ2wsx"
      MYSQL_ROOT_HOST: "%"
      MYSQL_DATABASE: whole_corp
      MYSQL_CHARSET: utf8mb4
      MYSQL_COLLATION: utf8mb4_general_ci
    volumes:
      - ./sql/my.cnf:/etc/mysql/conf.d/my.cnf
      - ./volumes/mysql_data:/var/lib/mysql
    ports:
      - "3307:3306"  # Expose MySQL on port 3306
    profiles:
      - search    
    networks:
      - bigdata-net
      
  # elasticsearch:
  #   build:
  #     context: ./search_system
  #     dockerfile: Dockerfile_es
  #   environment:
  #     - discovery.type=single-node
  #     - xpack.security.enabled=true  # Keep security enabled
  #     # - xpack.security.http.ssl.enabled=true
  #     # - xpack.security.http.ssl.keystore.path=certs/http.p12
  #     # - xpack.security.http.ssl.truststore.path=certs/http.p12
  #     - ELASTIC_PASSWORD=gAcstb8v-lFCVzCBC__a  # Set password manually
  #   volumes:
  #     - ./search_system/es_certs/http_ca.crt:/usr/share/elasticsearch/config/certs/http_ca.crt  # Mount certificates 
  #     - ./volumes/es_data:/usr/share/elasticsearch/data
  #   profiles:
  #     - search    
  #   depends_on:
  #     - mysql_db
  #   ports:
  #     - "9200:9200"
  #     - "9300:9300"
  #   networks:
  #     - bigdata-net

  # fastapi_search_service:
  #   build:
  #     context: ./search_system/FastAPI_service
  #     dockerfile: Dockerfile
  #   container_name: fastapi_search_service
  #   volumes:
  #     - ./search_system/es_certs:/usr/certs
  #     - ./search_system/FastAPI_service/code:/app  # Persistent data storage for FastAPI
  #   environment:
  #     - ES_USERNAME=elastic
  #     - ES_PASSWORD=gAcstb8v-lFCVzCBC__a  # Use the manually set password
  #     - ES_HOST=http://elasticsearch:9200
  #     - ES_CA_CERT=/usr/certs/http_ca.crt  # Optional, if you're using SSL
  #   networks:
  #     - bigdata-net
  #   profiles:
  #     - search    
  #   depends_on:
  #     - elasticsearch
  #   ports:
  #     - "3002:3002"  # Expose FastAPI on port 3002
  #     - "8501:8501"


  # --------------------
  # MinIO for object storage
  # --------------------
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./volumes/minio-data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    profiles: 
       - storage
    networks:
      - bigdata-net

  createbucket:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        sleep 5;
        mc alias set local http://minio:9000 minioadmin minioadmin; 
        mc mb -p local/deltabucket;
        mc mb -p local/media-bucket;
        mc policy set public local/deltabucket;
        mc policy set public local/media-bucket;
        exit 0;
      "
    profiles:
      - storage
    networks:
      - bigdata-net

  # --------------------
  # Spark Cluster
  # --------------------
  spark-master:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-master
    volumes:
      - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyterlab # For jupyterlab to test spark
      - ./airflow/include/spark_jobs:/usr/local/airflow/include/spark_jobs # For airflow to talk to spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
      - SPARK_MASTER_REST_ENABLED=true
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_REST_PORT=6066
    ports:
      - "7077:7077"
      # - "6066:6066"   # REST endpoint
      - "8081:8080"
      - "4040:4040"
    profiles:
      - spark
    networks:
      - bigdata-net

  spark-worker-1:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-1
    volumes:
      # - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyterlab
      - ./airflow/include/spark_jobs:/usr/local/airflow/include/spark_jobs
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=3G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles:
      - spark
    networks:
      - bigdata-net

  spark-worker-2:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-2
    volumes:
      # - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyterlab
      - ./airflow/include/spark_jobs:/usr/local/airflow/include/spark_jobs
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=3G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles:
      - spark
    networks:
      - bigdata-net

  jupyter:
    build:
      context: .
      dockerfile: spark_jobs/Dockerfile_jupyter
    container_name: jupyter
    volumes:
      - ./airflow/include/dependencies:/home/jovyan/work/dependencies
      - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyter
    environment:
      - GRANT_SUDO=yes
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
      - JUPYTER_ENABLE_LAB=yes
    ports:
      - "8888:8888"
      - "4041:4040"
    profiles:
      - spark
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --ip=0.0.0.0
    depends_on:
      - spark-master
    networks:
      - bigdata-net


networks:
  bigdata-net:
    external: true
