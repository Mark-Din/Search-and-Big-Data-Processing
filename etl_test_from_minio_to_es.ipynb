{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22956d71",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'minio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mminio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Minio\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'minio'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://localhost:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "MINIO_BUCKET = os.getenv(\"MINIO_BUCKET\", \"deltabucket\")\n",
    "MINIO_PREFIX = os.getenv(\"MINIO_PREFIX\", \"\")  # e.g. 'gold/wholeCorp_delta'\n",
    "\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ES_USERNAME = os.getenv(\"ES_USERNAME\")\n",
    "ES_PASSWORD = os.getenv(\"ES_PASSWORD\")\n",
    "ES_CA_CERT = os.getenv(\"ES_CA_CERT\")  # path or None\n",
    "ES_INDEX = os.getenv(\"ES_INDEX\", \"wholecorp\")\n",
    "\n",
    "CHUNKSIZE = int(os.getenv(\"CHUNKSIZE\", \"5000\"))  # rows per bulk batch\n",
    "\n",
    "# ---------- Clients ----------\n",
    "# s3fs uses the S3 API and works with MinIO\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=MINIO_ACCESS_KEY,\n",
    "    secret=MINIO_SECRET_KEY,\n",
    "    client_kwargs={\"endpoint_url\": MINIO_ENDPOINT},\n",
    ")\n",
    "\n",
    "# Elasticsearch client\n",
    "es_kwargs = {\"basic_auth\": (ES_USERNAME, ES_PASSWORD)} if ES_USERNAME else {}\n",
    "if ES_URL.startswith(\"https\"):\n",
    "    es_kwargs[\"verify_certs\"] = True if ES_CA_CERT else False\n",
    "    if ES_CA_CERT:\n",
    "        es_kwargs[\"ca_certs\"] = ES_CA_CERT\n",
    "\n",
    "es = Elasticsearch(ES_URL, **es_kwargs)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def ensure_index(es: Elasticsearch, index: str):\n",
    "    \"\"\"Create the index with a simple mapping if it doesn't exist.\"\"\"\n",
    "    if es.indices.exists(index=index):\n",
    "        return\n",
    "    body = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"dynamic\": True,\n",
    "            \"date_detection\": True,\n",
    "            \"dynamic_templates\": [\n",
    "                # treat *_at or *_date as dates when possible\n",
    "                {\"dates\": {\"match_pattern\": \"regex\", \"match\": \".*(_at|_date|Date|timestamp)$\",\n",
    "                           \"mapping\": {\"type\": \"date\", \"ignore_malformed\": True}}},\n",
    "                # numeric strings -> try as keywords by default (let dynamic handle numerics)\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index, body=body)\n",
    "\n",
    "def _stable_id(doc: Dict[str, Any]) -> str:\n",
    "    \"\"\"Create a stable _id to deduplicate. Customize to your schema.\"\"\"\n",
    "    raw = json.dumps(doc, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def dict_rows_from_csv(s3_path: str, chunksize: int) -> Iterator[Dict[str, Any]]:\n",
    "    with fs.open(s3_path, \"rb\") as f:\n",
    "        for chunk in pd.read_csv(f, chunksize=chunksize):\n",
    "            # (Optional) normalize/clean here\n",
    "            # e.g., convert 'updatedAt' to ISO\n",
    "            if \"updatedAt\" in chunk.columns:\n",
    "                chunk[\"updatedAt\"] = pd.to_datetime(chunk[\"updatedAt\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "            for record in chunk.to_dict(orient=\"records\"):\n",
    "                yield record\n",
    "\n",
    "def dict_rows_from_jsonl(s3_path: str) -> Iterator[Dict[str, Any]]:\n",
    "    with fs.open(s3_path, \"rb\") as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            rec = json.loads(line.decode(\"utf-8\"))\n",
    "            yield rec\n",
    "\n",
    "def dict_rows_from_parquet(s3_path: str, chunksize: int) -> Iterator[Dict[str, Any]]:\n",
    "    # Parquet isn't naturally chunked; load in frames then split (memory ok for moderate size).\n",
    "    # For huge data, consider PyArrow row groups iteration.\n",
    "    df = pd.read_parquet(f\"s3://{s3_path}\", storage_options={\n",
    "        \"key\": MINIO_ACCESS_KEY,\n",
    "        \"secret\": MINIO_SECRET_KEY,\n",
    "        \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT},\n",
    "    })\n",
    "    if \"updatedAt\" in df.columns:\n",
    "        df[\"updatedAt\"] = pd.to_datetime(df[\"updatedAt\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    if len(df) <= chunksize:\n",
    "        for r in df.to_dict(orient=\"records\"):\n",
    "            yield r\n",
    "    else:\n",
    "        for start in range(0, len(df), chunksize):\n",
    "            sub = df.iloc[start:start+chunksize]\n",
    "            for r in sub.to_dict(orient=\"records\"):\n",
    "                yield r\n",
    "\n",
    "def actions_from_docs(docs: Iterable[Dict[str, Any]], index: str) -> Iterator[Dict[str, Any]]:\n",
    "    for d in docs:\n",
    "        # (Optional) field remaps / type fixes\n",
    "        # Example: coerce numeric strings\n",
    "        # for k in (\"amount\",\"price\",\"count\"):\n",
    "        #     if k in d:\n",
    "        #         try: d[k] = float(d[k])\n",
    "        #         except: pass\n",
    "\n",
    "        yield {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index,\n",
    "            \"_id\": d.get(\"_id\") or _stable_id(d),\n",
    "            \"_source\": d,\n",
    "        }\n",
    "\n",
    "def s3_keys(bucket: str, prefix: str = \"\") -> Iterator[str]:\n",
    "    path = f\"{bucket}/{prefix}\".rstrip(\"/\")\n",
    "    for key in fs.find(path):\n",
    "        # fs.find returns full paths like 'bucket/key'\n",
    "        yield key\n",
    "\n",
    "# ---------- Main pump ----------\n",
    "def pump_object(key: str, index: str):\n",
    "    s3_path = key  # already like 'bucket/key.ext'\n",
    "    lower = s3_path.lower()\n",
    "    if lower.endswith(\".csv\"):\n",
    "        docs = dict_rows_from_csv(s3_path, chunksize=CHUNKSIZE)\n",
    "    elif lower.endswith(\".jsonl\") or lower.endswith(\".ndjson\"):\n",
    "        docs = dict_rows_from_jsonl(s3_path)\n",
    "    elif lower.endswith(\".parquet\"):\n",
    "        docs = dict_rows_from_parquet(s3_path, chunksize=CHUNKSIZE)\n",
    "    else:\n",
    "        print(f\"Skip unsupported file type: {s3_path}\")\n",
    "        return\n",
    "\n",
    "    # Stream to ES\n",
    "    success, fail = 0, 0\n",
    "    for ok, resp in streaming_bulk(es, actions_from_docs(docs, index=index), chunk_size=CHUNKSIZE, max_retries=3):\n",
    "        if ok:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    print(f\"[{s3_path}] bulk result: success={success}, failed={fail}\")\n",
    "\n",
    "def main():\n",
    "    ensure_index(es, ES_INDEX)\n",
    "    root = f\"{MINIO_BUCKET}/{MINIO_PREFIX}\" if MINIO_PREFIX else MINIO_BUCKET\n",
    "    keys = list(s3_keys(MINIO_BUCKET, MINIO_PREFIX))\n",
    "    if not keys:\n",
    "        print(f\"No objects found under s3://{root}\")\n",
    "        return\n",
    "    for key in keys:\n",
    "        pump_object(key, ES_INDEX)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c46471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       統一編號      公司名稱                                           features\n",
      "0  37838867    勁霸平價熱炒  {'type': 0, 'size': 262147, 'indices': [178355...\n",
      "1  37838873  北海讚日式涮涮鍋  {'type': 0, 'size': 262147, 'indices': [34950,...\n",
      "2  37839079   弘宸精品火鍋城  {'type': 0, 'size': 262147, 'indices': [178355...\n",
      "3  37839768  茉尼好食光早午餐  {'type': 0, 'size': 262147, 'indices': [178355...\n",
      "4  37840792    溫泉快炒小站  {'type': 0, 'size': 262147, 'indices': [178355...\n"
     ]
    }
   ],
   "source": [
    "import s3fs, pyarrow.parquet as pq, pandas as pd\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=\"minioadmin\", secret=\"minioadmin\",\n",
    "    client_kwargs={\"endpoint_url\": \"http://localhost:9000\"}\n",
    ")\n",
    "\n",
    "prefix = \"deltabucket/gold/wholeCorp_delta\"\n",
    "files = fs.glob(f\"{prefix}/*.parquet\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    table = pq.ParquetDataset(f\"s3://{f}\", filesystem=fs).read()\n",
    "    df = table.to_pandas()\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5158bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=\"minioadmin\", secret=\"minioadmin\",\n",
    "    client_kwargs={\"endpoint_url\": \"http://localhost:9000\"}\n",
    ")\n",
    "\n",
    "prefix = \"deltabucket/gold/wholeCorp_delta\"\n",
    "files = fs.glob(f\"{prefix}/*.parquet\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    table = pq.ParquetDataset(f\"s3://{f}\", filesystem=fs).read()\n",
    "    df = table.to_pandas()\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# convert \"features\" struct into dense numpy vector\n",
    "def to_dense(row):\n",
    "    if pd.isna(row):\n",
    "        return None\n",
    "    indices = row[\"indices\"]\n",
    "    values = row[\"values\"]\n",
    "    size = row[\"size\"]\n",
    "    dense = np.zeros(size, dtype=float)\n",
    "    dense[indices] = values\n",
    "    return dense.tolist()\n",
    "\n",
    "df[\"features_vector\"] = df[\"features\"].apply(to_dense)\n",
    "print(df[[\"公司名稱\",\"features_vector\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52394493",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elasticsearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Iterator, Dict, Any, Iterable\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01melasticsearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Elasticsearch\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01melasticsearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m streaming_bulk\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01ms3fs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'elasticsearch'"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Read Delta from MinIO ---\n",
    "storage_options = {\n",
    "    \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n",
    "    \"AWS_ENDPOINT_URL\": MINIO_ENDPOINT,\n",
    "}\n",
    "\n",
    "dt = DeltaTable(DELTA_PATH, storage_options=storage_options)\n",
    "\n",
    "# Convert to Pandas (you can chunk if too big)\n",
    "df = dt.to_pandas()\n",
    "\n",
    "# --- Send to Elasticsearch ---\n",
    "es = Elasticsearch(ES_URL)\n",
    "\n",
    "def docs():\n",
    "    for rec in df.to_dict(orient=\"records\"):\n",
    "        yield {\"_index\": ES_INDEX, \"_source\": rec}\n",
    "\n",
    "helpers.bulk(es, docs())\n",
    "print(f\"Inserted {len(df)} records into {ES_INDEX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b1f0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema string:\n",
      "{\"type\":\"struct\",\"fields\":[{\"name\":\"統一編號\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"公司名稱\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"features\",\"type\":{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}},\"nullable\":true,\"metadata\":{\"ml_attr\":{\"num_attrs\":262147}}}]}\n"
     ]
    }
   ],
   "source": [
    "import s3fs, json\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    key=\"minioadmin\",\n",
    "    secret=\"minioadmin\",\n",
    "    client_kwargs={\"endpoint_url\": \"http://localhost:9000\"},\n",
    ")\n",
    "\n",
    "log_path = \"deltabucket/gold/wholeCorp_delta/_delta_log/00000000000000000000.json\"\n",
    "\n",
    "with fs.open(log_path) as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        if \"metaData\" in obj:\n",
    "            print(\"Schema string:\")\n",
    "            print(obj[\"metaData\"][\"schemaString\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0885053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading s3fs-2025.7.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.24.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: fsspec==2025.7.0 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from s3fs) (2025.7.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs)\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.39.12,>=1.39.9 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.39.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Collecting multidict<7.0.0,>=6.0.0 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached propcache-0.3.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
      "  Using cached yarl-1.20.1-cp310-cp310-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from botocore<1.39.12,>=1.39.9->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from multidict<7.0.0,>=6.0.0->aiobotocore<3.0.0,>=2.5.4->s3fs) (4.14.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\miniconda3\\envs\\nexva_penta_integration_env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n",
      "Downloading s3fs-2025.7.0-py3-none-any.whl (30 kB)\n",
      "Downloading aiobotocore-2.24.1-py3-none-any.whl (85 kB)\n",
      "Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl (452 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading botocore-1.39.11-py3-none-any.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.9 MB 882.6 kB/s eta 0:00:16\n",
      "   - -------------------------------------- 0.5/13.9 MB 882.6 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 0.8/13.9 MB 958.5 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.3/13.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 1.2 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 2.4/13.9 MB 1.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.6/13.9 MB 1.3 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 3.1/13.9 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.7/13.9 MB 1.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.9/13.9 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 4.2/13.9 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 4.5/13.9 MB 1.5 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.7/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.7/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.0/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.2/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.5/13.9 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.8/13.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.3/13.9 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.6/13.9 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 7.1/13.9 MB 1.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.3/13.9 MB 1.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.9/13.9 MB 1.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 8.4/13.9 MB 1.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.7/13.9 MB 1.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 9.2/13.9 MB 1.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.4/13.9 MB 1.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.7/13.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.2/13.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 10.7/13.9 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 11.0/13.9 MB 1.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.5/13.9 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.1/13.9 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.8/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.4/13.9 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Using cached yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Using cached propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, async-timeout, aioitertools, aiohappyeyeballs, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
      "\n",
      "   ------------- --------------------------  4/12 [aioitertools]\n",
      "  Attempting uninstall: botocore\n",
      "   ------------- --------------------------  4/12 [aioitertools]\n",
      "    Found existing installation: botocore 1.40.17\n",
      "   ------------- --------------------------  4/12 [aioitertools]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "    Uninstalling botocore-1.40.17:\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "      Successfully uninstalled botocore-1.40.17\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ----------------------- ----------------  7/12 [botocore]\n",
      "   ------------------------------ ---------  9/12 [aiohttp]\n",
      "   ------------------------------ ---------  9/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiobotocore]\n",
      "   ---------------------------------------- 12/12 [s3fs]\n",
      "\n",
      "Successfully installed aiobotocore-2.24.1 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aioitertools-0.12.0 aiosignal-1.4.0 async-timeout-5.0.1 botocore-1.39.11 frozenlist-1.7.0 multidict-6.6.4 propcache-0.3.2 s3fs-2025.7.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.40.17 requires botocore<1.41.0,>=1.40.17, but you have botocore 1.39.11 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nexva_penta_integration_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
