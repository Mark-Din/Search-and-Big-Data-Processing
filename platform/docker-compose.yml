# # ---------------
# # search_system
# # ---------------
services:
  # For custome purpose
  mysql_db:
    build:
      context: .
      dockerfile: sql/Dockerfile
    container_name: mysql_db_container
    environment:
      MYSQL_ROOT_PASSWORD: "!QAZ2wsx"
      MYSQL_ROOT_HOST: "%"
      MYSQL_DATABASE: whole_corp
    command: [
      "--server-id=1",
      "--log-bin=mysql-bin",
      "--binlog-format=ROW",
      "--binlog-row-image=FULL"
    ]
    volumes:
      - ./volumes/mysql_data:/var/lib/mysql
    ports:
      - "3307:3306"  # Expose MySQL on port 3306
    profiles: ["search","test","dev","full"]    
    networks:
      - bigdata-net
  
  mysql:
    image: mysql:8
    container_name: hive_mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: metastore
    ports:
      - "3308:3306"
  networks:
    - bigdata-net

  elasticsearch:
    build:
      context: ./search_system
      dockerfile: Dockerfile_es
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true  # Keep security enabled
      # - xpack.security.http.ssl.enabled=true
      # - xpack.security.http.ssl.keystore.path=certs/http.p12
      # - xpack.security.http.ssl.truststore.path=certs/http.p12
      - ELASTIC_PASSWORD=gAcstb8v-lFCVzCBC__a  # Set password manually
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - ./search_system/es_certs/http_ca.crt:/usr/share/elasticsearch/config/certs/http_ca.crt  # Mount certificates 
      - ./volumes/es_data:/usr/share/elasticsearch/data  # Persistent data storage for Elasticsearch
    profiles: ["search","test","dev","full"]    
    # depends_on:
    #   - mysql_db
    ports:
      - "9202:9200"
    networks:
      - bigdata-net

  elastic_indices_init:
    image: curlimages/curl:latest
    build: 
      context: ./search_system
      dockerfile: Dockerfile_es_init
    command: ["bash", "-c", "/app/init_es.py"]
    networks:
      - bigdata-net
    profiles: ["search","test","dev","full"]    
  
  fastapi_search_service:
    build:
      context: ./search_system
      dockerfile: Dockerfile_py
    container_name: fastapi_search_service
    volumes:
      - ./search_system/es_certs:/usr/certs
      - ./search_system/FastAPI_service/code:/app  # Persistent data storage for FastAPI
    environment:
      - ES_USERNAME=elastic
      - ES_PASSWORD=gAcstb8v-lFCVzCBC__a  # Use the manually set password
      - ES_HOST=http://elasticsearch:9200
      - ES_CA_CERT=/usr/certs/http_ca.crt  # Optional, if you're using SSL
    networks:
      - bigdata-net
    profiles: ["search","test","dev","full"]    
    # depends_on:
    #   - elasticsearch
    ports:
      - "3002:3002"  # Expose FastAPI on port 3002
      - "8501:8501"


  # --------------------
  # MinIO for object storage
  # --------------------
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    volumes:
      - ./volumes/minio-data:/data
      - /mnt/ssd/minio-hot:/data
    command: server /data --console-address ":9001"
    profiles: ["storage","full"]    
    networks:
      - bigdata-net

  createbucket:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        sleep 5;
        mc alias set local http://minio:9000 minioadmin minioadmin; 
        mc mb -p local/deltabucket;
        mc mb -p local/media-bucket;
        mc policy set public local/deltabucket;
        mc policy set public local/media-bucket;
        exit 0;
      "
    profiles: ["storage","full"]    
    networks:
      - bigdata-net


  # --------------------
  # Kafka cluster
  # --------------------
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: ${kafka_main}
    environment:
      # KRaft mode enabled
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@${kafka_main}:9093

      # Listeners
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${kafka_main}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT

      # Basic settings
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

      CLUSTER_ID: it0xCINPS5ayG7l_DQRpDA
    ports:
      - "9092:9092"
    profiles: ["streaming","test","dev","full"]   
    networks:
      - bigdata-net

  kafka_init:
    build: 
      context: .
      dockerfile: kafka/Dockerfile_kafka_init
    depends_on: 
      - kafka
    command: ["bash", "-c", "/app/init_kafka.sh"]
    profiles: ["streaming","test","dev","full"]
    networks:
      - bigdata-net

  python-kafka-consumer:
    build:
      context: .
      dockerfile: kafka/Dockerfile_python_consumer
    container_name: kafka_python_consumer
    volumes:
      - ./kafka/python_kafka:/app/python
    depends_on:
      - kafka
    profiles: ["streaming","test","dev","full"]   
    networks:
      - bigdata-net


  kafka-connect:  # It will break if topics in kafka-qidu is not created
    build: 
      context: .
      dockerfile: kafka/Dockerfile_connect
    container_name: kafka-connect
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components
    ports:
      - "8083:8083"  # REST API for managing connectors
    depends_on:
      - kafka
    profiles: ["streaming","test","dev","full"]   
    networks:
      - bigdata-net

  # --------------------
  # Spark Cluster
  # --------------------
  spark-master:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-master
    command: >
      bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    # volumes:
    #   - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyterlab # For jupyterlab to test spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
      - SPARK_MASTER_REST_ENABLED=true
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_PORT=7077
      # - SPARK_MASTER_REST_PORT=6066
    ports:
      - "7077:7077"
      # - "6066:6066"   # REST endpoint
      - "8081:8080"
    # depends_on:
    #   - kafka
    #   - kafka-connect
    #   - elasticsearch
    #   - mysql_db
    profiles: ["spark","full"]
    networks:
      - bigdata-net

  spark-worker-1:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-1
    # volumes:
    #     - ./volumes/spark_volume/spark_code:/opt/bitnami/spark/work-dir
    depends_on:
      - spark-master
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles: ["spark","full"]
    networks:
      - bigdata-net

  spark-worker-2:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-2
    # volumes:
    #   - ./volumes/spark_volume/spark_code:/opt/bitnami/spark/work-dir
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles:
      - spark
    networks:
      - bigdata-net

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: spark_jobs/Dockerfile_jupyter
  #   container_name: jupyter
  #   volumes:
  #     # - ./airflow/include/dependencies:/home/jovyan/work/dependencies
  #     - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyter
  #   environment:
  #     - GRANT_SUDO=yes
  #     - PYSPARK_NO_DRIVER=true
  #     - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
  #     - JUPYTER_ENABLE_LAB=yes
  #   ports:
  #     - "8888:8888"
  #   profiles:
  #     - spark
  #   command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --ip=0.0.0.0
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - bigdata-net

  # --------------------
  # Hive metadata Cluster
  # --------------------


  # --------------------
  # Test Cluster
  # --------------------
  test-runner:
    image: python:3.11
    volumes:
      - .:/app
    working_dir: /app
    networks:
      - bigdata-net


networks:
  bigdata-net: