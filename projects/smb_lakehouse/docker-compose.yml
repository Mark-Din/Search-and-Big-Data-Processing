# # ---------------
# # search_system
# # ---------------
services:
  # For custome purpose
  mysql_db:
    build:
      context: .
      dockerfile: sql/Dockerfile
    container_name: mysql_db_container
    environment:
      MYSQL_ROOT_PASSWORD: "!QAZ2wsx"
      MYSQL_ROOT_HOST: "%"
      MYSQL_DATABASE: whole_corp
    command: [
      "--server-id=1",
      "--log-bin=mysql-bin",
      "--binlog-format=ROW",
      "--binlog-row-image=FULL"
    ]
    volumes:
      # - ./sql/my.cnf:/etc/mysql/conf.d/my.cnf
      - mysql_data:/var/lib/mysql
    ports:
      - "3307:3306"  # Expose MySQL on port 3306
    profiles:
      - search    
    networks:
      - bigdata-net
      
  elasticsearch:
    build:
      context: ./search_system
      dockerfile: Dockerfile_es
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true  # Keep security enabled
      # - xpack.security.http.ssl.enabled=true
      # - xpack.security.http.ssl.keystore.path=certs/http.p12
      # - xpack.security.http.ssl.truststore.path=certs/http.p12
      - ELASTIC_PASSWORD=gAcstb8v-lFCVzCBC__a  # Set password manually
    volumes:
      - ./search_system/es_certs/http_ca.crt:/usr/share/elasticsearch/config/certs/http_ca.crt  # Mount certificates 
      - ./volumes/es_data:/usr/share/elasticsearch/data
    profiles:
      - search    
    depends_on:
      - mysql_db
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - bigdata-net

  fastapi_search_service:
    build:
      context: ./search_system/FastAPI_service
      dockerfile: Dockerfile
    container_name: fastapi_search_service
    volumes:
      - ./search_system/es_certs:/usr/certs
      - ./search_system/FastAPI_service/code:/app  # Persistent data storage for FastAPI
    environment:
      - ES_USERNAME=elastic
      - ES_PASSWORD=gAcstb8v-lFCVzCBC__a  # Use the manually set password
      - ES_HOST=http://elasticsearch:9200
      - ES_CA_CERT=/usr/certs/http_ca.crt  # Optional, if you're using SSL
    networks:
      - bigdata-net
    profiles:
      - search    
    depends_on:
      - elasticsearch
    ports:
      - "3002:3002"  # Expose FastAPI on port 3002
      - "8501:8501"


  # --------------------
  # MinIO for object storage
  # --------------------
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./volumes/minio-data:/data
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    profiles: 
       - storage
    networks:
      - bigdata-net

  createbucket:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
        sleep 5;
        mc alias set local http://minio:9000 minioadmin minioadmin; 
        mc mb -p local/deltabucket;
        mc mb -p local/media-bucket;
        mc policy set public local/deltabucket;
        mc policy set public local/media-bucket;
        exit 0;
      "
    profiles:
      - storage
    networks:
      - bigdata-net

  # --------------------
  # Kafka
  # --------------------
  kafka:
    build:
      context: .
      dockerfile: kafka/Dockerfile
    container_name: kafka
    environment:
      # KRaft mode enabled
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # Listeners
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT

      # Basic settings
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

      CLUSTER_ID: it0xCINPS5ayG7l_DQRpDA
    ports:
      - "9092:9092"
    profiles:
      - streaming
    networks:
      - bigdata-net

  kafka-python-consumer:
    build:
      context: .
      dockerfile: kafka/Dockerfile_python_consumer
    container_name: kafka_python_consumer
    depends_on:
      - kafka
      # - elasticsearch
    profiles:
      - streaming
    networks:
      - bigdata-net


  kafka-connect:
    build: 
      context: .
      dockerfile: kafka/Dockerfile_connect
    container_name: kafka-connect
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_GROUP_ID: "connect-cluster"
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components
    ports:
      - "8083:8083"  # REST API for managing connectors
    depends_on:
      - kafka
    profiles:
      - streaming
    networks:
      - bigdata-net

  # --------------------
  # Spark Cluster
  # --------------------
  spark-master:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-master
    command: >
      bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"
    volumes:
      - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyterlab # For jupyterlab to test spark
      - ./volumes/spark_volume/spark_code:/opt/bitnami/spark/work-dir
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
      - SPARK_MASTER_REST_ENABLED=true
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_PORT=7077
      # - SPARK_MASTER_REST_PORT=6066
    ports:
      - "7077:7077"
      # - "6066:6066"   # REST endpoint
      - "8081:8080"
    # depends_on:
    #   - kafka
    #   - kafka-connect
    #   - elasticsearch
    #   - mysql_db
    profiles:
      - spark
    networks:
      - bigdata-net

  spark-worker-1:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-1
    volumes:
        - ./volumes/spark_volume/spark_code:/opt/bitnami/spark/work-dir
    depends_on:
      - spark-master
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles:
      - spark
    networks:
      - bigdata-net

  spark-worker-2:
    build:
      context: .   # root of repo
      dockerfile: spark_jobs/Dockerfile
    container_name: spark-worker-2
    volumes:
      - ./volumes/spark_volume/spark_code:/opt/bitnami/spark/work-dir
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER_HOST=spark-master
      - PYSPARK_NO_DRIVER=true
      - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
    profiles:
      - spark
    networks:
      - bigdata-net

  # jupyter:
  #   build:
  #     context: .
  #     dockerfile: spark_jobs/Dockerfile_jupyter
  #   container_name: jupyter
  #   volumes:
  #     # - ./airflow/include/dependencies:/home/jovyan/work/dependencies
  #     - ./volumes/spark_volume/jupyterlab:/home/jovyan/work/jupyter
  #   environment:
  #     - GRANT_SUDO=yes
  #     - PYSPARK_NO_DRIVER=true
  #     - PYSPARK_SUBMIT_ARGS=--jars /opt/bitnami/spark/jars/delta-core_2.12-3.1.0.jar,/opt/bitnami/spark/jars/delta-storage-3.1.0.jar pyspark-shell
  #     - JUPYTER_ENABLE_LAB=yes
  #   ports:
  #     - "8888:8888"
  #   profiles:
  #     - spark
  #   command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --ip=0.0.0.0
  #   depends_on:
  #     - spark-master
  #   networks:
  #     - bigdata-net


volumes:
  mysql_data:
    external: true

networks:
  bigdata-net:
    external: true
    
