{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab5be44-fccc-4989-afe6-d78a0fa6ac38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c198a7da-4f9b-4ffe-a0c8-f0bdf57684f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"1\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "s = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38762a96-81a3-4089-abd4-c0bf9498268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gold = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "df = s.read.format(\"delta\").load(gold)\n",
    "\n",
    "# Label from 公司狀態 → Active=1 else 0 (adjust mapping as needed)\n",
    "base = s.read.format(\"delta\").load(os.getenv(\"SILVER_PATH\",\"s3a://deltabucket/silver/wholeCorp_delta\"))\\\n",
    "        .select(\"統一編號\",\"公司狀態\")\n",
    "data = df.join(base, \"統一編號\", \"left\")\n",
    "data = data.withColumn(\"label\", F.when(F.col(\"公司狀態\")==\"核准設立\", 1).otherwise(0))\n",
    "\n",
    "# Split\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "s.conf.set(\"spark.sql.shuffle.partitions\", \"50\")  # was 200\n",
    "train = train.select(\"label\",\"features\").repartition(50)\\\n",
    "             .persist(StorageLevel.MEMORY_AND_DISK)\n",
    "train.count()  # materialize\n",
    "paramGrid = ParamGridBuilder().build() # no grid first cv = CrossValidator(estimator=clf, estimatorParamMaps=paramGrid,\n",
    "clf = GBTClassifier(labelCol=\"label\", featuresCol=\"features\",\n",
    "                    maxDepth=4, maxIter=30, subsamplingRate=0.8,\n",
    "                    maxBins=32, cacheNodeIds=True)\n",
    "\n",
    "evaluator=BinaryClassificationEvaluator(labelCol=\"label\",\n",
    "                                        rawPredictionCol=\"rawPrediction\",\n",
    "                                        metricName=\"areaUnderPR\")\n",
    "tvs = TrainValidationSplit(estimator=clf,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8, parallelism=2)\n",
    "\n",
    "best = tvs.fit(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5f614b-557c-490b-90d7-e0b52cf59774",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best.transform(test)\n",
    "print(\"AUPRC:\", evaluator.evaluate(preds))\n",
    "\n",
    "# Save model path for scoring\n",
    "out_uri = os.getenv(\"STATUS_MODEL_URI\",\"s3a://deltabucket/models/status_gbt\")\n",
    "best.bestModel.write().overwrite().save(out_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211cddb-1268-4e7f-8d1a-1bd908566114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
