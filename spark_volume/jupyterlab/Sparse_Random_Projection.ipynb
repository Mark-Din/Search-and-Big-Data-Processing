{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9278d2b-7789-42e8-a1cd-a41e59a6d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b011ce0-f5c0-42a5-9a1f-97a4bfc1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"2\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f680133b-9c66-4a76-ae5f-2e88382840b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MiniBatchKMeans] Reassigning 12 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n"
     ]
    }
   ],
   "source": [
    "# --- Spark session (reuse yours)\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters\")\n",
    "\n",
    "data = spark.read.format(\"delta\").load(GOLD).select(\"features\")\n",
    "data_whole = spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "# --- get feature size from one row\n",
    "first_vec = data.limit(1).collect()[0][0]\n",
    "num_features = int(first_vec.size)\n",
    "\n",
    "# --- helper: Spark Row -> CSR batch\n",
    "def to_csr(rows):\n",
    "    indptr = [0]; indices = []; vals = []\n",
    "    for r in rows:\n",
    "        sv = r[\"features\"]\n",
    "        indices.extend(sv.indices.tolist())\n",
    "        vals.extend(sv.values.tolist())\n",
    "        indptr.append(indptr[-1] + len(sv.indices))\n",
    "    return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                       np.array(indices, dtype=np.int32),\n",
    "                       np.array(indptr, dtype=np.int32)),\n",
    "                      shape=(len(rows), num_features\n",
    "))\n",
    "\n",
    "# # Define a UDF to extract values\n",
    "# def extract_indices(vector):\n",
    "#     if isinstance(vector, SparseVector):\n",
    "#         return vector.indices.tolist()\n",
    "#     return None\n",
    "\n",
    "# def extract_values(vector):\n",
    "#     if isinstance(vector, SparseVector):\n",
    "#         return vector.values.tolist()\n",
    "#     return None\n",
    "    \n",
    "# extract_indices_udf = F.udf(extract_indices, ArrayType(IntegerType()))\n",
    "# extract_values_udf = F.udf(extract_values, ArrayType(DoubleType()))\n",
    "\n",
    "# # Apply the UDF to extract the values as a list\n",
    "# df_with_values = data.withColumn(\"indices_list\", extract_indices_udf(\"features\"))\n",
    "# df_with_values = df_with_values.withColumn(\"values_list\", extract_values_udf(\"features\"))\n",
    "\n",
    "# 1) collect a manageable sample from Spark\n",
    "sample_rows = []\n",
    "for i, row in enumerate(data.sample(False, 0.02, seed=42).toLocalIterator()):  # ~2% example\n",
    "    sample_rows.append(row)\n",
    "    if i >= 20000:      # cap by count if you like\n",
    "        break\n",
    "\n",
    "X_sample = to_csr(sample_rows)            # CSR (n_sample, num_features)\n",
    "svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "# --- Stage 2: fit MiniBatchKMeans on reduced features\n",
    "kmeans = MiniBatchKMeans(n_clusters=15,\n",
    "                         random_state=42,\n",
    "                         batch_size=2000,\n",
    "                         verbose=1,\n",
    "                         n_init='auto')\n",
    "chunks = []\n",
    "batch = []\n",
    "for row in data.toLocalIterator():\n",
    "    batch.append(row)\n",
    "    if len(batch) >= 2000:\n",
    "        Xb = to_csr(batch)\n",
    "        Xr = svd.transform(Xb)            # reduced batch\n",
    "        kmeans.partial_fit(Xr)\n",
    "        chunks.append(Xr)\n",
    "        batch.clear()\n",
    "if batch:\n",
    "    Xb = to_csr(batch); Xr = svd.transform(Xb); kmeans.partial_fit(Xr)\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "decebe06-595e-4406-9d4b-cba12ad124b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_whole = spark.read.format(\"delta\").load(GOLD).select(\"features\")\n",
    "data_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34118ba7-42e6-4c0d-9a8e-ae9474e0f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = spark.read.format(\"delta\").load(GOLD).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41837924-65ba-496a-9265-73ade2936209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[7956,262...|\n",
      "|(262147,[80719,26...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[117068,2...|\n",
      "|(262147,[171092,2...|\n",
      "|(262147,[256426,2...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[171092,2...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[3201,262...|\n",
      "|(262147,[247690,2...|\n",
      "|(262147,[76079,26...|\n",
      "|(262147,[76079,26...|\n",
      "|(262147,[240732,2...|\n",
      "|(262147,[63958,26...|\n",
      "|(262147,[98664,26...|\n",
      "|(262147,[36506,26...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e91ee20-9523-4973-bf89-f898035c9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 2000\n",
    "rows_buf, ids_buf = [], []\n",
    "\n",
    "# (Re)create output table empty (optional)\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS delta.`{OUT}`\")  # only if you want a fresh table\n",
    "\n",
    "for r in data_whole.toLocalIterator():\n",
    "    rows_buf.append(r)\n",
    "    ids_buf.append(r[\"統一編號\"])\n",
    "    if len(rows_buf) >= BATCH:\n",
    "        Xb = to_csr(rows_buf)\n",
    "        Xr = svd.transform(Xb)\n",
    "        yb = kmeans.predict(Xr).astype(int)\n",
    "\n",
    "        pdf = pd.DataFrame({\"統一編號\": ids_buf, \"cluster\": yb})\n",
    "        spark.createDataFrame(pdf).write.format(\"delta\").mode(\"append\").save(OUT)\n",
    "\n",
    "        rows_buf.clear(); ids_buf.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4a368ef-871c-4077-a2b5-65783df6f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rows_buf:\n",
    "    Xb = to_csr(rows_buf)\n",
    "    Xr = svd.transform(Xb)\n",
    "    yb = kmeans.predict(Xr).astype(int)\n",
    "    pdf = pd.DataFrame({\"統一編號\": ids_buf, \"cluster\": yb})\n",
    "    spark.createDataFrame(pdf).write.format(\"delta\").mode(\"append\").save(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ca78d9a-d219-4af3-8db1-3a427e8222a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = spark.read.format(\"delta\").load('s3a://deltabucket/gold/wholeCorp_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab0f3a3-958f-4842-8763-4e1aa5c19464",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_Xr = svd.transform(Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a672413-1918-48d5-918b-06ea369b491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca138a-95cc-437b-bfca-143c62dc9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle, pandas as pd, numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "# serialize models (TruncatedSVD / SRP + MiniBatchKMeans)\n",
    "proj_blob   = cloudpickle.dumps(svd)      # or srp\n",
    "kmeans_blob = cloudpickle.dumps(kmeans)\n",
    "\n",
    "schema = T.StructType([\n",
    "  T.StructField(\"統一編號\", T.StringType(), False),\n",
    "  T.StructField(\"cluster\",  T.IntegerType(), True),\n",
    "])\n",
    "\n",
    "@F.pandas_udf(schema, \"MAP_ITER\")\n",
    "def predict_clusters(it):\n",
    "    import cloudpickle, numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "    proj   = cloudpickle.loads(proj_blob)\n",
    "    kmeans = cloudpickle.loads(kmeans_blob)\n",
    "    for pdf in it:\n",
    "        ids  = pdf[\"統一編號\"].tolist()\n",
    "        vecs = pdf[\"features\"].tolist()\n",
    "\n",
    "        # build CSR for this partition\n",
    "        indptr, indices, data = [0], [], []\n",
    "        size = vecs[0].size if vecs else 0\n",
    "        for v in vecs:\n",
    "            if isinstance(v, SparseVector):\n",
    "                indices.extend(v.indices); data.extend(v.values)\n",
    "                indptr.append(indptr[-1] + len(v.indices))\n",
    "            else:\n",
    "                arr = np.asarray(v.toArray()); nz = arr.nonzero()[0]\n",
    "                indices.extend(nz); data.extend(arr[nz]); indptr.append(indptr[-1]+len(nz))\n",
    "        X = csr_matrix((np.array(data, float), np.array(indices, int), np.array(indptr, int)),\n",
    "                       shape=(len(vecs), size))\n",
    "\n",
    "        Z = proj.transform(X)             # SVD/SRP transform in bulk\n",
    "        y = kmeans.predict(Z).astype(int)\n",
    "        yield pd.DataFrame({\"統一編號\": ids, \"cluster\": y})\n",
    "\n",
    "full = (spark.read.format(\"delta\").load(GOLD)\n",
    "        .select(\"統一編號\",\"features\")\n",
    "        .repartition(200))                 # tune to cluster size\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "pred = full.mapInPandas(predict_clusters, schema)\n",
    "\n",
    "(pred\n",
    " .repartition(64)                          # avoid many small files\n",
    " .write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\",\"true\")\n",
    " .save(OUT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60e6c1-2e46-4df6-92da-ad5f8cb22e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f50925-b2cb-4b74-ac5c-52a0ae850656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963e311-5444-4df5-ad5d-e7b339bac067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6461fc2-e1f2-4e98-9d3f-86075aa0ce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1786"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d180c513-d157-439e-8cae-5413999865aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2, silhouette=0.979\n",
      "k=3, silhouette=0.926\n",
      "k=4, silhouette=0.923\n",
      "k=5, silhouette=0.927\n",
      "k=6, silhouette=0.929\n",
      "k=7, silhouette=0.932\n",
      "k=8, silhouette=0.947\n",
      "k=9, silhouette=0.956\n",
      "k=10, silhouette=0.957\n",
      "k=11, silhouette=0.957\n",
      "k=12, silhouette=0.960\n",
      "k=13, silhouette=0.961\n",
      "k=14, silhouette=0.960\n",
      "k=15, silhouette=0.962\n",
      "k=16, silhouette=0.960\n",
      "k=17, silhouette=0.963\n",
      "k=18, silhouette=0.963\n",
      "k=19, silhouette=0.963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "for k in range(2,20):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(Xr)\n",
    "    km.fit(Xr)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    score = silhouette_score(Xr, km.labels_)\n",
    "    print(f\"k={k}, silhouette={score:0.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32d428-292f-4908-bf9d-8b4b419bbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357df54-36d2-48ce-87b4-70734f5c15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(kmeans.predict(Xr).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1e24e-75b2-489a-b0e4-4a8d7b1dc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if batch:\n",
    "    Xb = to_csr(batch); chunks.append(svd.transform(Xb))\n",
    "    \n",
    "# ... after you fill `chunks` with SRP-transformed CSR batches:\n",
    "X_all = sp_vstack(chunks, format=\"csr\")   # stack sparsely, stays CSR\n",
    "\n",
    "# nn = NearestNeighbors(n_neighbors=10, metric=\"cosine\", algorithm=\"brute\").fit(X_all)\n",
    "nn = NearestNeighbors(n_neighbors=10, metric=\"cosine\").fit(X_all)\n",
    "\n",
    "# --- Example: query nearest neighbors for company i=123\n",
    "distances, indices = nn.kneighbors(X_all[123].reshape(1,-1))\n",
    "print(\"Nearest neighbor indices:\", indices, \"distances:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e90f5c4e-4a2b-4603-9d9e-375e9fc3db12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After you build X_all (CSR)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nnz \u001b[38;5;241m=\u001b[39m \u001b[43mX_all\u001b[49m\u001b[38;5;241m.\u001b[39mgetnnz(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)              \u001b[38;5;66;03m# number of nonzeros per row\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mint\u001b[39m((nnz\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# If you’re querying row i:\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "# After you build X_all (CSR)\n",
    "nnz = X_all.getnnz(axis=1)              # number of nonzeros per row\n",
    "print(\"Zero rows:\", int((nnz==0).sum()))\n",
    "\n",
    "# If you’re querying row i:\n",
    "i = 123\n",
    "print(\"Query nnz:\", int(X_all[i].getnnz()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19eb5d54-c740-4d87-a21c-11fe0cbaa490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers shape: (10, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kmeans_10.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cluster centers shape:\", kmeans.cluster_centers_.shape)\n",
    "\n",
    "# --- Save both\n",
    "joblib.dump(svd, \"svd_100.joblib\")\n",
    "joblib.dump(kmeans, \"kmeans_10.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d232d8-0dcc-49f3-acca-e0680eb2dead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c632e8-65c6-425b-b3e2-d13b34cdf851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
