{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35f61b2-b57b-4120-a63e-72fc240e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        # Resources\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"512m\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.pandas as ps\n",
    "from sparksession import spark_session\n",
    "\n",
    "# Read Silver data\n",
    "def read_silver(spark):\n",
    "    silver = os.getenv(\"SILVER_PATH\",\"s3a://deltabucket/silver/wholeCorp_delta\")\n",
    "    return spark.read.format(\"delta\").load(silver)\n",
    "\n",
    "# Basic flags\n",
    "def vevtorize(df):\n",
    "    def has(col): return (F.col(col).isNotNull() & (F.length(F.col(col))>0)).cast(\"int\")\n",
    "    df = (df\n",
    "          .withColumn(\"has_官網\", has(\"官網\"))\n",
    "          .withColumn(\"has_電話\", has(\"電話\"))\n",
    "          .withColumn(\"log_資本額\", F.log1p(F.col(\"資本額\")))\n",
    "    )\n",
    "    df = df.fillna({\"log_資本額\": 0})\n",
    "    \n",
    "    # Text\n",
    "    text_col = F.coalesce(F.col(\"類別_全\"))\n",
    "    df = df.withColumn(\"text_all\", text_col)\n",
    "    df = df.withColumn(\"text_str\", F.col(\"text_all\").cast(\"string\"))\n",
    "    df = df.fillna({\"text_str\": \"\"})  # or drop: df = df.dropna(subset=[\"text_str\"])\n",
    "    \n",
    "    tok = RegexTokenizer(inputCol=\"text_str\", outputCol=\"tok\",\n",
    "                         pattern=\"\\\\s+\", gaps=True, toLowercase=True)\n",
    "    stop = StopWordsRemover(inputCol=\"tok\", outputCol=\"tok_clean\")\n",
    "    tf = HashingTF(inputCol=\"tok_clean\", outputCol=\"tf\", numFeatures=1<<18)\n",
    "    idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "    \n",
    "    # Categorical\n",
    "    cats = []\n",
    "    for c in [\"縣市名稱\", \"區域名稱\", \"上市櫃_基本資料\"]:\n",
    "        if c in df.columns: cats.append(c)\n",
    "    \n",
    "    # indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cats]\n",
    "    # encoders = [OneHotEncoder(inputCols=[f\"{c}_idx\"], outputCols=[f\"{c}_ohe\"]) for c in cats]\n",
    "    \n",
    "    num_cols = [c for c in [\"log_資本額\",\"log_實收資本總額\"] if c in df.columns]\n",
    "    bin_cols = [\"has_官網\",\"has_電話\"]\n",
    "    # ohe_cols = [f\"{c}_ohe\" for c in cats]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[\"tfidf\"] + num_cols + bin_cols,\n",
    "                                outputCol=\"features_raw\")\n",
    "    scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "    \n",
    "    pipe = Pipeline(stages=[tok, stop, tf, idf]+ [assembler, scaler])\n",
    "    model = pipe.fit(df)\n",
    "    out = model.transform(df)\n",
    "    return out\n",
    "    \n",
    "def save_gold(out):\n",
    "    gold = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    (out.select(\"統一編號\",\"公司名稱\",\"features\")\n",
    "        .write.format(\"delta\").mode(\"overwrite\").save(gold))\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        s = spark_session()\n",
    "        df = read_silver(s)\n",
    "        out = vevtorize(df)\n",
    "        save_gold(out)\n",
    "        s.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spark job: {e}\")\n",
    "    finally:\n",
    "        if 's' in locals():\n",
    "            s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b8cf933-5bb6-4a27-b414-3cc2e5c3a0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(DISTINCT 官網)=32656)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(F.countDistinct(\"官網\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c05efc0a-1d9e-49c1-9488-7af02aa8d624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        log_資本額|\n",
      "+------------------+\n",
      "| 16.41820031748273|\n",
      "|16.759949589762293|\n",
      "|14.436087583323648|\n",
      "| 18.72078534381011|\n",
      "|17.196301163869315|\n",
      "|13.104201443434462|\n",
      "| 16.32103657658766|\n",
      "|13.791589722477323|\n",
      "|14.845523112142669|\n",
      "|13.222914345709215|\n",
      "| 16.90609840800616|\n",
      "|15.452563831983857|\n",
      "|15.292559510663105|\n",
      "|16.120093753421386|\n",
      "| 14.19394767861579|\n",
      "| 9.169622538697624|\n",
      "|17.491811255187805|\n",
      "| 19.34493964944395|\n",
      "|13.998665933701242|\n",
      "|17.212997363947366|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"log_資本額\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5d84f37-cb31-4992-b52c-f3366a5cc962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_資本額 nulls: 261305\n",
      "log_實收資本總額 nulls: 1218984\n",
      "has_官網 nulls: 0\n",
      "has_聯絡人 nulls: 0\n",
      "has_電話 nulls: 0\n",
      "has_手機 nulls: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "inputs = [\"log_資本額\",\"log_實收資本總額\",\"has_官網\",\"has_聯絡人\",\"has_電話\",\"has_手機\"]\n",
    "\n",
    "for c in inputs:\n",
    "    n = df.filter(F.col(c).isNull()).count()\n",
    "    print(c, \"nulls:\", n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
