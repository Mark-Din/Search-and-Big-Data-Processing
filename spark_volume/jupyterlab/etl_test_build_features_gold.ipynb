{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f61b2-b57b-4120-a63e-72fc240e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, StandardScaler\n",
    "\n",
    "zh_stopwords = [\n",
    "        \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\",\n",
    "        \"都\", \"一\", \"上\", \"也\", \"很\", \"到\", \"他\", \"年\", \"就是\", \"而\",\n",
    "        \"我們\", \"這個\", \"可以\", \"這些\", \"自己\", \"沒有\", \"這樣\", \"著\",\n",
    "        \"多\", \"對\", \"下\", \"但\", \"要\", \"被\", \"讓\", \"她\", \"向\", \"以\",\n",
    "        \"所以\", \"把\", \"跟\", \"之\", \"其\", \"又\", \"在這裡\", \"這\", \"能\",\n",
    "        \"應該\", \"則\", \"然後\", \"只是\", \"那\", \"在那裡\", \"這種\", \"因為\",\n",
    "        \"這是\", \"而且\", \"如何\", \"誰\", \"它\", \"不是\", \"這裡\", \"如此\",\n",
    "        \"每個\", \"這一點\", \"即使\", \"大\", \"小\", \"因此\", \"可能\", \"其他\",\n",
    "        \"不過\", \"他們\", \"最後\", \"使用\", \"至於\", \"此\", \"其中\", \"大家\",\n",
    "        \"或者\", \"最\", \"且\", \"雖然\", \"那麼\", \"這些\", \"一些\", \"通過\",\n",
    "        \"為什麼\", \"什麼\", \"進行\", \"再\", \"已經\", \"不同\", \"整個\", \"以及\",\n",
    "        \"從\", \"這樣的\", \"不能\", \"他的\", \"我們的\", \"自\", \"這邊\", \"那邊\",\n",
    "        \"對於\", \"所有\", \"能夠\", \"請\", \"給\", \"在此\", \"上面\", \"以下\",\n",
    "        \"儘管\", \"不需要\", \"不管\", \"與此同時\", \"關於\", \"有關\", \"將\",\n",
    "        \"沒事\", \"沒關係\", \"這邊\", \"那邊\", \"有時候\", \"有時\", \"為\", \"可能性\"\n",
    "]\n",
    "\n",
    "# --- IO ---\n",
    "def read_silver(spark):\n",
    "    silver = os.getenv(\"SILVER_PATH\",\"s3a://deltabucket/silver/wholeCorp_delta\")\n",
    "    return spark.read.format(\"delta\").load(silver)\n",
    "\n",
    "def save_gold(df):\n",
    "    gold = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    (df.select(\"統一編號\",\"公司名稱\",\"features\")\n",
    "       .write.format(\"delta\").mode(\"overwrite\").save(gold))\n",
    "\n",
    "# --- Fit + Transform ---\n",
    "def vectorize(df, zh_stopwords:list=None):\n",
    "    def has(col): return (F.col(col).isNotNull() & (F.length(F.col(col)) > 0)).cast(\"int\")\n",
    "\n",
    "    df = (df\n",
    "          .withColumn(\"has_官網\", has(\"官網\"))\n",
    "          .withColumn(\"has_電話\", has(\"電話\"))\n",
    "          .withColumn(\"log_資本額\", F.log1p(F.col(\"資本額\")))\n",
    "          .fillna({\"log_資本額\": 0})\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"text_str\", F.coalesce(F.col(\"類別_全\").cast(\"string\"), F.lit(\"\")))\n",
    "\n",
    "    tok  = RegexTokenizer(inputCol=\"text_str\", outputCol=\"tok\", pattern=\"\\\\s+\", gaps=True, toLowercase=True)\n",
    "    stop = StopWordsRemover(inputCol=\"tok\", outputCol=\"tok_clean\")\n",
    "    if zh_stopwords:\n",
    "        stop = stop.setStopWords(stop.getStopWords() + zh_stopwords)\n",
    "\n",
    "    tf   = HashingTF(inputCol=\"tok_clean\", outputCol=\"tf\", numFeatures=1<<18)\n",
    "    idf  = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "    num_cols = [c for c in [\"log_資本額\",\"log_實收資本總額\"] if c in df.columns]\n",
    "    bin_cols = [\"has_官網\",\"has_電話\"]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[\"tfidf\"] + num_cols + bin_cols, outputCol=\"features_raw\")\n",
    "    scaler    = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")  # withMean=False by default\n",
    "\n",
    "    pipe = Pipeline(stages=[tok, stop, tf, idf, assembler, scaler])\n",
    "    model = pipe.fit(df)\n",
    "    out   = model.transform(df)\n",
    "    return out, model\n",
    "\n",
    "# --- Export learned params for Python ETL (no Spark needed later) ---\n",
    "def export_params_to_json(model, out_path=\"s3a://deltabucket/models/sparseVector_params/model_params.json\"):\n",
    "    stages  = model.stages\n",
    "    tfm     = next(s for s in stages if s.__class__.__name__ == \"HashingTF\")\n",
    "    idfm    = next(s for s in stages if s.__class__.__name__ == \"IDFModel\")\n",
    "    scalerm = next(s for s in stages if s.__class__.__name__ == \"StandardScalerModel\")\n",
    "\n",
    "    payload = {\n",
    "        \"num_features\": tfm.getNumFeatures(),\n",
    "        \"with_mean\": scalerm.getWithMean(),\n",
    "        \"with_std\":  scalerm.getWithStd(),\n",
    "        \"idf\": idfm.idf.toArray().tolist(),\n",
    "        \"std\": scalerm.std.toArray().tolist(),\n",
    "        \"mean\": scalerm.mean.toArray().tolist() if scalerm.getWithMean() else None,\n",
    "        \"dr\": None,         # fill later if you add PCA/SVD\n",
    "        \"kmeans\": None      # fill later if you add KMeans\n",
    "    }\n",
    "\n",
    "    # Write a single JSON file to MinIO (Spark way)\n",
    "    j = json.dumps(payload)\n",
    "    model.sparkSession.sparkContext.parallelize([j], 1).saveAsTextFile(out_path)\n",
    "\n",
    "def main():\n",
    "    from sparksession import spark_session\n",
    "    s = None\n",
    "    try:\n",
    "        s = spark_session()\n",
    "        df = read_silver(s)\n",
    "        out, model = vectorize(df, zh_stopwords=[])\n",
    "\n",
    "        # 1) Save the Spark model for Spark-side reuse\n",
    "        model.write().overwrite().save(\"s3a://deltabucket/models/sparseVector\")\n",
    "\n",
    "        # 2) Export minimal params for Python-only ETL\n",
    "        export_params_to_json(model, \"s3a://deltabucket/models/sparseVector_params\")\n",
    "\n",
    "        # 3) Save features to GOLD (optional)\n",
    "        save_gold(out)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spark job: {e}\")\n",
    "    finally:\n",
    "        if s: s.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
