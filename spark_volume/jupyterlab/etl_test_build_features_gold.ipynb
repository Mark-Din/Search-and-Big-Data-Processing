{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a66907-6d3c-4803-a1be-db5c7c84cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e4c333-a6de-482e-b331-6e6b81359f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"2\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        # .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35f61b2-b57b-4120-a63e-72fc240e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pathlib\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, StandardScaler\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "zh_stopwords = [\n",
    "        \"的\", \"了\", \"在\", \"是\", \"我\", \"有\", \"和\", \"就\", \"不\", \"人\",\n",
    "        \"都\", \"一\", \"上\", \"也\", \"很\", \"到\", \"他\", \"年\", \"就是\", \"而\",\n",
    "        \"我們\", \"這個\", \"可以\", \"這些\", \"自己\", \"沒有\", \"這樣\", \"著\",\n",
    "        \"多\", \"對\", \"下\", \"但\", \"要\", \"被\", \"讓\", \"她\", \"向\", \"以\",\n",
    "        \"所以\", \"把\", \"跟\", \"之\", \"其\", \"又\", \"在這裡\", \"這\", \"能\",\n",
    "        \"應該\", \"則\", \"然後\", \"只是\", \"那\", \"在那裡\", \"這種\", \"因為\",\n",
    "        \"這是\", \"而且\", \"如何\", \"誰\", \"它\", \"不是\", \"這裡\", \"如此\",\n",
    "        \"每個\", \"這一點\", \"即使\", \"大\", \"小\", \"因此\", \"可能\", \"其他\",\n",
    "        \"不過\", \"他們\", \"最後\", \"使用\", \"至於\", \"此\", \"其中\", \"大家\",\n",
    "        \"或者\", \"最\", \"且\", \"雖然\", \"那麼\", \"這些\", \"一些\", \"通過\",\n",
    "        \"為什麼\", \"什麼\", \"進行\", \"再\", \"已經\", \"不同\", \"整個\", \"以及\",\n",
    "        \"從\", \"這樣的\", \"不能\", \"他的\", \"我們的\", \"自\", \"這邊\", \"那邊\",\n",
    "        \"對於\", \"所有\", \"能夠\", \"請\", \"給\", \"在此\", \"上面\", \"以下\",\n",
    "        \"儘管\", \"不需要\", \"不管\", \"與此同時\", \"關於\", \"有關\", \"將\",\n",
    "        \"沒事\", \"沒關係\", \"這邊\", \"那邊\", \"有時候\", \"有時\", \"為\", \"可能性\"\n",
    "]\n",
    "\n",
    "# --- IO ---\n",
    "def read_silver(spark):\n",
    "    silver = os.getenv(\"SILVER_PATH\",\"s3a://deltabucket/silver/wholeCorp_delta\")\n",
    "    return spark.read.format(\"delta\").load(silver)\n",
    "\n",
    "def save_gold(df):\n",
    "    gold = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    (df.select(\"統一編號\",\"公司名稱\",\"features\")\n",
    "       .write.format(\"delta\").mode(\"overwrite\").save(gold))\n",
    "\n",
    "# --- Fit + Transform ---\n",
    "def vectorize(df, zh_stopwords:list=None):\n",
    "    def has(col): return (F.col(col).isNotNull() & (F.length(F.col(col)) > 0)).cast(\"int\")\n",
    "\n",
    "    df = (df\n",
    "          .withColumn(\"has_官網\", has(\"官網\"))\n",
    "          .withColumn(\"has_電話\", has(\"電話\"))\n",
    "          .withColumn(\"log_資本額\", F.log1p(F.col(\"資本額\")))\n",
    "          .fillna({\"log_資本額\": 0})\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"text_str\", F.coalesce(F.col(\"類別_全\").cast(\"string\"), F.lit(\"\")))\n",
    "\n",
    "    tok  = RegexTokenizer(inputCol=\"text_str\", outputCol=\"tok\", pattern=\"\\\\s+\", gaps=True, toLowercase=True)\n",
    "    stop = StopWordsRemover(inputCol=\"tok\", outputCol=\"tok_clean\")\n",
    "    if zh_stopwords:\n",
    "        stop = stop.setStopWords(stop.getStopWords() + zh_stopwords)\n",
    "\n",
    "    tf   = HashingTF(inputCol=\"tok_clean\", outputCol=\"tf\", numFeatures=1<<15) # numFeatures is for creating dimensions, it means 2**15\n",
    "    idf  = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "\n",
    "    num_cols = [c for c in [\"log_資本額\",\"log_實收資本總額\"] if c in df.columns]\n",
    "    bin_cols = [\"has_官網\",\"has_電話\"]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[\"tfidf\"] + num_cols + bin_cols, outputCol=\"features_raw\")\n",
    "    scaler    = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")  # withMean=False by default\n",
    "\n",
    "    pipe = Pipeline(stages=[tok, stop, tf, idf, assembler, scaler])\n",
    "    model = pipe.fit(df)\n",
    "    out   = model.transform(df)\n",
    "    return out, model\n",
    "\n",
    "# --- Export learned params for Python ETL (no Spark needed later) ---\n",
    "def export_params_to_minio(model, s3a_uri=\"s3a://deltabucket/models/sparseVector_params/model_params.json\"):\n",
    "    tfm  = next(s for s in model.stages if s.__class__.__name__ == \"HashingTF\")\n",
    "    scal = next(s for s in model.stages if s.__class__.__name__ == \"StandardScalerModel\")\n",
    "\n",
    "    payload  = {\n",
    "        \"num_features\": tfm.getNumFeatures(),\n",
    "        \"with_mean\":    scal.getWithMean(),\n",
    "        \"with_std\":     scal.getWithStd(),\n",
    "    }\n",
    "\n",
    "    \n",
    "    u = urlparse(s3a_uri.replace(\"s3a://\", \"s3://\"))\n",
    "    bucket = u.netloc\n",
    "    key    = u.path.lstrip(\"/\")\n",
    "    if not key or key.endswith(\"/\"):\n",
    "        key = key.rstrip(\"/\") + \"/model_params.json\"  # default file name\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url='http://minio:9000',\n",
    "        aws_access_key_id='minioadmin',\n",
    "        aws_secret_access_key='minioadmin'\n",
    "    )\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket, Key=key,\n",
    "                  Body=json.dumps(payload, ensure_ascii=False).encode(\"utf-8\"),\n",
    "                  ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    s = spark_session()\n",
    "    try:\n",
    "        df = read_silver(s)\n",
    "        out, model = vectorize(df, zh_stopwords=[])\n",
    "\n",
    "        # 1) Save the Spark model for Spark-side reuse\n",
    "        model.write().overwrite().save(\"s3a://deltabucket/models/sparseVector\")\n",
    "\n",
    "        # 2) Export minimal params for Python-only ETL\n",
    "        export_params_to_minio(model, \"s3a://deltabucket/models/sparseVector_params/model_params.json\")\n",
    "\n",
    "        # 3) Save features to GOLD (optional)\n",
    "        save_gold(out)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spark job: {e}\")\n",
    "    finally:\n",
    "        if s: s.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980ba32-1087-4ecf-9ff0-aedd5b957dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
