{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9278d2b-7789-42e8-a1cd-a41e59a6d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b011ce0-f5c0-42a5-9a1f-97a4bfc1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    # \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                    \"org.postgresql:postgresql:42.7.3\"\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"2\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d60bd71-e84f-47b2-979b-bf57564d0a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "def fit_predict(data_whole, sample_frac=0.005, sample_cap=5000, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Run SVD + MiniBatchKMeans on a manageable sample.\n",
    "    Returns: Spark DataFrame [統一編號, cluster, Xr_vector]\n",
    "    \"\"\"\n",
    "\n",
    "    # --- get feature size from one row\n",
    "    first_vec = data_whole.limit(1).collect()[0][2]\n",
    "    num_features = int(first_vec.size)\n",
    "\n",
    "    # --- helper: Spark Row -> CSR batch\n",
    "    def to_csr(rows):\n",
    "        indptr = [0]; indices = []; vals = []\n",
    "        for sv in rows:\n",
    "            indices.extend(sv.indices.tolist())\n",
    "            vals.extend(sv.values.tolist())\n",
    "            indptr.append(indptr[-1] + len(sv.indices))\n",
    "        return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                           np.array(indices, dtype=np.int32),\n",
    "                           np.array(indptr, dtype=np.int32)),\n",
    "                          shape=(len(rows), num_features))\n",
    "\n",
    "    # 1) collect a small sample for SVD training\n",
    "    sample_rows = []\n",
    "    for i, row in enumerate(data_whole.sample(False, sample_frac, seed=42).toLocalIterator()):\n",
    "        sample_rows.append(row['features'])\n",
    "        if i >= sample_cap:   # hard cap\n",
    "            break\n",
    "\n",
    "    X_sample = to_csr(sample_rows)\n",
    "    print(f\"Sample size for SVD: {X_sample.shape}\")\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "    # --- Stage 2: incremental clustering\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=15,\n",
    "        random_state=42,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        n_init='auto'\n",
    "    )\n",
    "\n",
    "    ids, labels, Xr_vector = [], [], []\n",
    "    batch_features, batch_ids = [], []\n",
    "\n",
    "    for row in data_whole.select(\"統一編號\", \"features\").toLocalIterator():\n",
    "        batch_features.append(row[\"features\"])\n",
    "        batch_ids.append(row[\"統一編號\"])\n",
    "        if len(batch_features) >= batch_size:\n",
    "            Xb = to_csr(batch_features)\n",
    "            Xr = svd.transform(Xb)\n",
    "            Xr_vector.extend(Xr)\n",
    "\n",
    "            kmeans.partial_fit(Xr)\n",
    "            preds = kmeans.predict(Xr)\n",
    "\n",
    "            ids.extend(batch_ids)\n",
    "            labels.extend(preds.tolist())\n",
    "            batch_features.clear(); batch_ids.clear()\n",
    "\n",
    "    # last leftover batch\n",
    "    if batch_features:\n",
    "        Xb = to_csr(batch_features)\n",
    "        Xr = svd.transform(Xb)\n",
    "        Xr_vector.extend(Xr)\n",
    "\n",
    "        kmeans.partial_fit(Xr)\n",
    "        preds = kmeans.predict(Xr)\n",
    "        ids.extend(batch_ids)\n",
    "        labels.extend(preds.tolist())\n",
    "\n",
    "    # --- return as Spark DataFrame\n",
    "    rows = [\n",
    "        Row(\n",
    "            統一編號=str(i),\n",
    "            cluster=int(c),\n",
    "            Xr_vector=[float(v) for v in x.tolist()]\n",
    "        )\n",
    "        for i, c, x in zip(ids, labels, Xr_vector)\n",
    "    ]\n",
    "\n",
    "    spark = data_whole.sparkSession\n",
    "    sdf = spark.createDataFrame(rows)\n",
    "\n",
    "    return sdf, svd, kmeans\n",
    "\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    to_pg(spark, pdf)\n",
    "    \n",
    "    # s3 = boto3.client(\n",
    "    #     \"s3\",\n",
    "    #     endpoint_url = 'http://minio:9000',\n",
    "    #     aws_access_key_id=\"minioadmin\",\n",
    "    #     aws_secret_access_key=\"minioadmin\"\n",
    "    # )\n",
    "\n",
    "    # # save locally\n",
    "    # joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    # joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # # upload to MinIO\n",
    "    # s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    # s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")\n",
    "    \n",
    "# from sparksession import spark_session\n",
    "s = None\n",
    "s = spark_session()\n",
    "df = read_gold(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015c8ca-f5eb-4cd8-9c65-d81733b47417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for SVD: (5001, 32771)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_predict(data_whole, sample_frac=0.005, sample_cap=5000, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Run SVD + MiniBatchKMeans on a manageable sample.\n",
    "    Returns: Spark DataFrame [統一編號, cluster, Xr_vector]\n",
    "    \"\"\"\n",
    "\n",
    "    # --- get feature size from one row\n",
    "    first_vec = data_whole.limit(1).collect()[0][2]\n",
    "    num_features = int(first_vec.size)\n",
    "\n",
    "    # --- helper: Spark Row -> CSR batch\n",
    "    def to_csr(rows):\n",
    "        indptr = [0]; indices = []; vals = []\n",
    "        for sv in rows:\n",
    "            indices.extend(sv.indices.tolist())\n",
    "            vals.extend(sv.values.tolist())\n",
    "            indptr.append(indptr[-1] + len(sv.indices))\n",
    "        return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                           np.array(indices, dtype=np.int32),\n",
    "                           np.array(indptr, dtype=np.int32)),\n",
    "                          shape=(len(rows), num_features))\n",
    "\n",
    "    # 1) collect a small sample for SVD training\n",
    "    sample_rows = []\n",
    "    for i, row in enumerate(data_whole.sample(False, sample_frac, seed=42).toLocalIterator()):\n",
    "        sample_rows.append(row['features'])\n",
    "        if i >= sample_cap:   # hard cap\n",
    "            break\n",
    "\n",
    "    X_sample = to_csr(sample_rows)\n",
    "    print(f\"Sample size for SVD: {X_sample.shape}\")\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "    # # --- Stage 2: incremental clustering\n",
    "    # kmeans = MiniBatchKMeans(\n",
    "    #     n_clusters=15,\n",
    "    #     random_state=42,\n",
    "    #     batch_size=batch_size,\n",
    "    #     verbose=1,\n",
    "    #     n_init='auto'\n",
    "    # )\n",
    "\n",
    "    ids, labels, Xr_vector = [], [], []\n",
    "    batch_features, batch_ids = [], []\n",
    "\n",
    "    for row in data_whole.select(\"統一編號\", \"features\").toLocalIterator():\n",
    "        batch_features.append(row[\"features\"])\n",
    "        batch_ids.append(row[\"統一編號\"])\n",
    "        \n",
    "        if len(batch_features) >= batch_size:\n",
    "            print(f\"number {}{len(batch_features)}\")\n",
    "            Xb = to_csr(batch_features)\n",
    "            Xr = svd.transform(Xb)\n",
    "            Xr_vector.extend(Xr)\n",
    "\n",
    "            # kmeans.partial_fit(Xr)\n",
    "            # preds = kmeans.predict(Xr)\n",
    "\n",
    "            # ids.extend(batch_ids)\n",
    "            # labels.extend(preds.tolist())\n",
    "            # batch_features.clear(); batch_ids.clear()\n",
    "\n",
    "    # last leftover batch\n",
    "    if batch_features:\n",
    "        Xb = to_csr(batch_features)\n",
    "        Xr = svd.transform(Xb)\n",
    "        Xr_vector.extend(Xr)\n",
    "\n",
    "        # kmeans.partial_fit(Xr)\n",
    "        # preds = kmeans.predict(Xr)\n",
    "        # ids.extend(batch_ids)\n",
    "        # labels.extend(preds.tolist())\n",
    "\n",
    "    # --- return as Spark DataFrame\n",
    "    rows = [\n",
    "        Row(\n",
    "            統一編號=str(i),\n",
    "            cluster=int(c),\n",
    "            Xr_vector=[float(v) for v in x.tolist()]\n",
    "        )\n",
    "        for i, c, x in zip(ids, labels, Xr_vector)\n",
    "    ]\n",
    "\n",
    "    spark = data_whole.sparkSession\n",
    "    sdf = spark.createDataFrame(rows)\n",
    "\n",
    "    return sdf, svd, kmeans\n",
    "\n",
    "sdf, svd, kmeans = fit_predict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d758dde1-90d9-4684-b8a5-d0574d096a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Can't get JDBC type for struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Stop the SparkSession\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m---> 44\u001b[0m \u001b[43mto_pg\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m, in \u001b[0;36mto_pg\u001b[0;34m(spark, pdf)\u001b[0m\n\u001b[1;32m     15\u001b[0m pg_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfopower\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 4. Write DataFrame to PostgreSQL\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# The 'mode' specifies the behavior if the table already exists.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# \"overwrite\": Drops and recreates the table.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# \"append\": Adds data to the existing table.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# \"ignore\": Does nothing if the table exists.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# \"error\" or \"errorifexists\": Throws an error if the table exists (default).\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpg_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpg_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpg_properties\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully wrote DataFrame to PostgreSQL table \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpg_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# You can also read it back to verify\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Can't get JDBC type for struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "def to_pg(spark, sdf):\n",
    "    pg_url = \"jdbc:postgresql://pg_vector:5432/vector_db\"\n",
    "    pg_table = \"wholecorp_clusters_vector\"\n",
    "\n",
    "    pg_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"infopower\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "\n",
    "    # Tune these knobs depending on DB size/resources\n",
    "    num_partitions = 8     # how many parallel writers\n",
    "    batch_size = 5000      # rows per batch insert\n",
    "\n",
    "    (sdf.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", pg_url)\n",
    "        .option(\"dbtable\", pg_table)\n",
    "        .option(\"user\", pg_properties[\"user\"])\n",
    "        .option(\"password\", pg_properties[\"password\"])\n",
    "        .option(\"driver\", pg_properties[\"driver\"])\n",
    "        .option(\"batchsize\", batch_size)\n",
    "        .option(\"numPartitions\", num_partitions)\n",
    "        .option(\"truncate\", True)       # ensures overwrite works cleanly\n",
    "        .mode(\"overwrite\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Successfully wrote DataFrame to PostgreSQL table '{pg_table}'\")\n",
    "\n",
    "    # Verify readback\n",
    "    read_df = (spark.read\n",
    "               .format(\"jdbc\")\n",
    "               .option(\"url\", pg_url)\n",
    "               .option(\"dbtable\", pg_table)\n",
    "               .option(\"user\", pg_properties[\"user\"])\n",
    "               .option(\"password\", pg_properties[\"password\"])\n",
    "               .option(\"driver\", pg_properties[\"driver\"])\n",
    "               .load())\n",
    "    read_df.show(5)\n",
    "\n",
    "\n",
    "to_pg(s, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "278b5f9f-c044-4480-a7c0-58c7d7bec9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters_vector\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    spark.createDataFrame(pdf).write.format(\"delta\").mode(\"overwrite\").save(OUT)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url = 'http://minio:9000',\n",
    "        aws_access_key_id=\"minioadmin\",\n",
    "        aws_secret_access_key=\"minioadmin\"\n",
    "    )\n",
    "\n",
    "    # save locally\n",
    "    joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # upload to MinIO\n",
    "    s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69196176-519b-4cf7-9fe0-5714fcce8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"統一編號\"] = pdf[\"統一編號\"].astype(str)\n",
    "pdf[\"Xr_vector\"] = pdf[\"Xr_vector\"].apply(\n",
    "    lambda v: [float(x) for x in (v.tolist() if isinstance(v, np.ndarray) else v)]\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"統一編號\", StringType(), False),\n",
    "    StructField(\"cluster\", IntegerType(), True),\n",
    "    StructField(\"Xr_vector\", ArrayType(DoubleType()), True),\n",
    "])\n",
    "\n",
    "sdf = spark.createDataFrame(pdf.to_dict(orient=\"records\"), schema=schema)\n",
    "sdf.write.format(\"delta\").mode(\"overwrite\").save(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796e2b2-9152-4c58-8e07-0889c7a60bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr /= (np.linalg.norm(Xr, axis=1, keepdims=True) + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0267525-57cf-4fc6-8d03-79ba515702d9",
   "metadata": {},
   "source": [
    "# Metrics to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031865c-9eff-48cd-b74a-732dfe9e3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "for k in range(2,20):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(Xr)\n",
    "    km.fit(Xr)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    score = silhouette_score(Xr, km.labels_)\n",
    "    print(f\"k={k}, silhouette={score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4a1ab-a9ce-4a76-9f2c-2c24a9d69b04",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
