{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9278d2b-7789-42e8-a1cd-a41e59a6d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b011ce0-f5c0-42a5-9a1f-97a4bfc1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"2\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d60bd71-e84f-47b2-979b-bf57564d0a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11356)\t37.40266267386384\n",
      "  (0, 32768)\t2.2077088848113995\n",
      "  (0, 32769)\t0.0\n",
      "  (0, 32770)\t0.0\n",
      "  (1, 677)\t104.23746404352207\n",
      "  (1, 32768)\t2.440939021179923\n",
      "  (1, 32769)\t0.0\n",
      "  (1, 32770)\t0.0\n",
      "  (2, 31190)\t9.459186080170428\n",
      "  (2, 32768)\t2.324323910935715\n",
      "  (2, 32769)\t0.0\n",
      "  (2, 32770)\t0.0\n",
      "  (3, 31190)\t9.459186080170428\n",
      "  (3, 32768)\t2.440939021179923\n",
      "  (3, 32769)\t0.0\n",
      "  (3, 32770)\t0.0\n",
      "  (4, 31190)\t9.459186080170428\n",
      "  (4, 32768)\t2.4716128155207024\n",
      "  (4, 32769)\t0.0\n",
      "  (4, 32770)\t0.0\n",
      "  (5, 31190)\t9.459186080170428\n",
      "  (5, 32768)\t1.8203246226517424\n",
      "  (5, 32769)\t0.0\n",
      "  (5, 32770)\t0.0\n",
      "  (6, 360)\t14.22791115141308\n",
      "  :\t:\n",
      "  (19998, 13986)\t12.850234088562502\n",
      "  (19998, 14467)\t15.735769353841357\n",
      "  (19998, 14981)\t26.37058174062191\n",
      "  (19998, 16433)\t11.706956967466958\n",
      "  (19998, 19253)\t24.41822558547225\n",
      "  (19998, 20139)\t27.5295722953701\n",
      "  (19998, 20652)\t37.158925234059694\n",
      "  (19998, 21722)\t16.107109732966386\n",
      "  (19998, 24285)\t25.166452785139843\n",
      "  (19998, 25492)\t12.700286825449568\n",
      "  (19998, 26099)\t22.682721577906214\n",
      "  (19998, 27132)\t10.916059154583126\n",
      "  (19998, 29112)\t13.140926926884443\n",
      "  (19998, 30346)\t6.9285810198554625\n",
      "  (19998, 32768)\t2.7423848555257093\n",
      "  (19998, 32769)\t0.0\n",
      "  (19998, 32770)\t0.0\n",
      "  (19999, 27628)\t18.276845420900198\n",
      "  (19999, 29505)\t4.519262913151475\n",
      "  (19999, 32769)\t0.0\n",
      "  (19999, 32770)\t0.0\n",
      "  (20000, 27556)\t303.29303763326527\n",
      "  (20000, 32768)\t2.7117110499689248\n",
      "  (20000, 32769)\t0.0\n",
      "  (20000, 32770)\t0.0\n",
      "[MiniBatchKMeans] Reassigning 14 cluster centers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "\n",
    "def fit_predict(data_whole):\n",
    "    # --- get feature size from one row\n",
    "    first_vec = data_whole.limit(1).collect()[0][2]\n",
    "    num_features = int(first_vec.size)\n",
    "\n",
    "    # --- helper: Spark Row -> CSR batch\n",
    "    def to_csr(rows):\n",
    "        indptr = [0]; indices = []; vals = []\n",
    "        for sv in rows:\n",
    "            indices.extend(sv.indices.tolist())\n",
    "            vals.extend(sv.values.tolist())\n",
    "            indptr.append(indptr[-1] + len(sv.indices))\n",
    "        return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                        np.array(indices, dtype=np.int32),\n",
    "                        np.array(indptr, dtype=np.int32)),\n",
    "                        shape=(len(rows), num_features\n",
    "    ))\n",
    "\n",
    "    # 1) collect a manageable sample from Spark\n",
    "    sample_rows = []\n",
    "    for i, row in enumerate(data_whole.sample(False, 0.02, seed=42).toLocalIterator()):  # ~2% example\n",
    "        sample_rows.append(row['features'])\n",
    "        if i >= 20000:      # cap by count if you like\n",
    "            break\n",
    "\n",
    "    X_sample = to_csr(sample_rows)            # CSR (n_sample, num_features)\n",
    "    print(X_sample)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "    # --- Stage 2: fit MiniBatchKMeans on reduced features\n",
    "    kmeans = MiniBatchKMeans(n_clusters=15,\n",
    "                            random_state=42,\n",
    "                            batch_size=2000,\n",
    "                            verbose=1,\n",
    "                            n_init='auto')\n",
    "    ids, labels = [], []\n",
    "    batch_features, batch_ids = [], []\n",
    "\n",
    "    Xr_vector = []\n",
    "    for row in data_whole.select(\"統一編號\",\"features\").toLocalIterator():\n",
    "        batch_features.append(row[\"features\"])\n",
    "        batch_ids.append(row[\"統一編號\"])\n",
    "        if len(batch_features) >= 2000:\n",
    "            Xb = to_csr(batch_features)\n",
    "            Xr = svd.transform(Xb)\n",
    "            Xr_vector.extend(Xr)\n",
    "            \n",
    "            kmeans.partial_fit(Xr)\n",
    "            preds = kmeans.predict(Xr)            # ndarray\n",
    "            ids.extend(batch_ids)                  # flatten ids\n",
    "            labels.extend(preds.tolist())          # flatten labels\n",
    "            batch_features.clear(); batch_ids.clear()\n",
    "\n",
    "    if batch_features:\n",
    "        Xb = to_csr(batch_features)\n",
    "        Xr = svd.transform(Xb)\n",
    "        Xr_vector.extend(Xr)\n",
    "        \n",
    "        kmeans.partial_fit(Xr)\n",
    "        preds = kmeans.predict(Xr)\n",
    "        ids.extend(batch_ids)\n",
    "        labels.extend(preds.tolist())\n",
    "\n",
    "    pdf = pd.DataFrame({\"統一編號\": ids, \"cluster\": labels, \"Xr_vector\":Xr_vector})\n",
    "\n",
    "    return pdf, svd, kmeans, Xr_vector\n",
    "\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    spark.createDataFrame(pdf).write.format(\"delta\").mode(\"overwrite\").save(OUT)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url = 'http://minio:9000',\n",
    "        aws_access_key_id=\"minioadmin\",\n",
    "        aws_secret_access_key=\"minioadmin\"\n",
    "    )\n",
    "\n",
    "    # save locally\n",
    "    joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # upload to MinIO\n",
    "    s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")\n",
    "\n",
    "# from sparksession import spark_session\n",
    "s = None\n",
    "s = spark_session()\n",
    "df = read_gold(s)\n",
    "pdf, svd, kmeans, Xr_vector = fit_predict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "278b5f9f-c044-4480-a7c0-58c7d7bec9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters_vector\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    spark.createDataFrame(pdf).write.format(\"delta\").mode(\"overwrite\").save(OUT)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url = 'http://minio:9000',\n",
    "        aws_access_key_id=\"minioadmin\",\n",
    "        aws_secret_access_key=\"minioadmin\"\n",
    "    )\n",
    "\n",
    "    # save locally\n",
    "    joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # upload to MinIO\n",
    "    s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69196176-519b-4cf7-9fe0-5714fcce8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf[\"統一編號\"] = pdf[\"統一編號\"].astype(str)\n",
    "pdf[\"Xr_vector\"] = pdf[\"Xr_vector\"].apply(\n",
    "    lambda v: [float(x) for x in (v.tolist() if isinstance(v, np.ndarray) else v)]\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"統一編號\", StringType(), False),\n",
    "    StructField(\"cluster\", IntegerType(), True),\n",
    "    StructField(\"Xr_vector\", ArrayType(DoubleType()), True),\n",
    "])\n",
    "\n",
    "sdf = spark.createDataFrame(pdf.to_dict(orient=\"records\"), schema=schema)\n",
    "sdf.write.format(\"delta\").mode(\"overwrite\").save(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d758dde1-90d9-4684-b8a5-d0574d096a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `Xr_vector`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py:1630\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1673\u001b[0m     )\n\u001b[1;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py:1681\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m         StructField(\n\u001b[1;32m   1680\u001b[0m             k,\n\u001b[0;32m-> 1681\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py:1636\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1637\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvd\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 102\u001b[0m, in \u001b[0;36msave_\u001b[0;34m(pdf, spark, kmeans, svd)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_\u001b[39m(pdf, spark, kmeans, svd):\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(OUT)\n\u001b[1;32m    104\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m         endpoint_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://minio:9000\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    107\u001b[0m         aws_access_key_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminioadmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m         aws_secret_access_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminioadmin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# save locally\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:955\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 955\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:958\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m--> 958\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/types.py:1691\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m             StructField(\n\u001b[1;32m   1680\u001b[0m                 k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1688\u001b[0m             )\n\u001b[1;32m   1689\u001b[0m         )\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1692\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1693\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: k},\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `Xr_vector`."
     ]
    }
   ],
   "source": [
    "save_(pdf, s, kmeans, svd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0799b1-be3d-42b2-accf-b21f8d578e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[vector.tolist() for i in Xr_vector for vector in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8ac62b-b7a1-4605-81e5-cf55117509ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d7b18-0f88-429b-858d-ee0656b366dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- get feature size from one row\n",
    "first_vec = data_whole.limit(1).collect()[0][2]\n",
    "num_features = int(first_vec.size)\n",
    "\n",
    "# --- helper: Spark Row -> CSR batch\n",
    "def to_csr(rows):\n",
    "    indptr = [0]; indices = []; vals = []\n",
    "    for sv in rows:\n",
    "        indices.extend(sv.indices.tolist())\n",
    "        vals.extend(sv.values.tolist())\n",
    "        indptr.append(indptr[-1] + len(sv.indices))\n",
    "    return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                    np.array(indices, dtype=np.int32),\n",
    "                    np.array(indptr, dtype=np.int32)),\n",
    "                    shape=(len(rows), num_features\n",
    "))\n",
    "\n",
    "# 1) collect a manageable sample from Spark\n",
    "sample_rows = []\n",
    "for i, row in enumerate(data_whole.sample(False, 0.02, seed=42).toLocalIterator()):  # ~2% example\n",
    "    sample_rows.append(row['features'])\n",
    "    if i >= 20000:      # cap by count if you like\n",
    "        break\n",
    "\n",
    "X_sample = to_csr(sample_rows)            # CSR (n_sample, num_features)\n",
    "svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "# --- Stage 2: fit MiniBatchKMeans on reduced features\n",
    "kmeans = MiniBatchKMeans(n_clusters=15,\n",
    "                        random_state=42,\n",
    "                        batch_size=2000,\n",
    "                        verbose=1,\n",
    "                        n_init='auto')\n",
    "ids, labels = [], []\n",
    "batch_features, batch_ids = [], []\n",
    "\n",
    "for row in data_whole.select(\"統一編號\",\"features\").toLocalIterator():\n",
    "    batch_features.append(row[\"features\"])\n",
    "    batch_ids.append(row[\"統一編號\"])\n",
    "    if len(batch_features) >= 2000:\n",
    "        Xb = to_csr(batch_features)\n",
    "        Xr = svd.transform(Xb)\n",
    "\n",
    "        Xr /= (np.linalg.norm(Xr, axis=1, keepdims=True) + 1e-12)\n",
    "        \n",
    "        kmeans.partial_fit(Xr)\n",
    "\n",
    "        preds = kmeans.predict(Xr)            # ndarray\n",
    "        ids.extend(batch_ids)                  # flatten ids\n",
    "        labels.extend(preds.tolist())          # flatten labels\n",
    "        batch_features.clear(); batch_ids.clear()\n",
    "\n",
    "if batch_features:\n",
    "    Xb = to_csr(batch_features)\n",
    "    Xr = svd.transform(Xb)\n",
    "    kmeans.partial_fit(Xr)\n",
    "    preds = kmeans.predict(Xr)\n",
    "    ids.extend(batch_ids)\n",
    "    labels.extend(preds.tolist())\n",
    "\n",
    "pdf = pd.DataFrame({\"統一編號\": ids, \"cluster\": labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28b8c0a-b9cd-4490-bbac-df26e2dbd101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(262147, {63958: 9.4622, 262144: 2.4409, 262145: 0.0, 262146: 0.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).collect()[0][\"features\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937a0b6-5a0f-4563-a8c3-8f1fe4cb32e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796e2b2-9152-4c58-8e07-0889c7a60bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr /= (np.linalg.norm(Xr, axis=1, keepdims=True) + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0267525-57cf-4fc6-8d03-79ba515702d9",
   "metadata": {},
   "source": [
    "# Metrics to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031865c-9eff-48cd-b74a-732dfe9e3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "for k in range(2,20):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(Xr)\n",
    "    km.fit(Xr)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    score = silhouette_score(Xr, km.labels_)\n",
    "    print(f\"k={k}, silhouette={score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4a1ab-a9ce-4a76-9f2c-2c24a9d69b04",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
