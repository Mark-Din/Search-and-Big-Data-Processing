{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9278d2b-7789-42e8-a1cd-a41e59a6d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b011ce0-f5c0-42a5-9a1f-97a4bfc1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"1\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        # .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60bd71-e84f-47b2-979b-bf57564d0a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "\n",
    "def to_mysql(ids, labels, Xr_vector, spark, batch_size):\n",
    "    \n",
    "    rows = [\n",
    "        Row(\n",
    "            統一編號=i,\n",
    "            cluster=c,\n",
    "            vector=json.dumps(x)\n",
    "        ) for i,c,x in zip(ids, labels, Xr_vector)\n",
    "    ]\n",
    "        \n",
    "    sdf = spark.createDataFrame(rows)\n",
    "    \n",
    "    mysql_url = \"jdbc:mysql://mysql_db_container:3306/whole_corp\"\n",
    "    mysql_table = \"wholecorp_clusters_vector\"\n",
    "\n",
    "    mysql_properties = {\n",
    "        \"user\": \"root\",\n",
    "        \"password\": \"!QAZ2wsx\",\n",
    "        \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "\n",
    "    num_partitions = 8\n",
    "\n",
    "    (sdf.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", mysql_url)\n",
    "        .option(\"dbtable\", mysql_table)\n",
    "        .option(\"user\", mysql_properties[\"user\"])\n",
    "        .option(\"password\", mysql_properties[\"password\"])\n",
    "        .option(\"driver\", mysql_properties[\"driver\"])\n",
    "        .option(\"batchsize\", batch_size)\n",
    "        .option(\"numPartitions\", num_partitions)\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Successfully wrote DataFrame to MySQL table '{mysql_table}'\")\n",
    "\n",
    "    # read_df = (spark.read\n",
    "    #            .format(\"jdbc\")\n",
    "    #            .option(\"url\", mysql_url)\n",
    "    #            .option(\"dbtable\", mysql_table)\n",
    "    #            .option(\"user\", mysql_properties[\"user\"])\n",
    "    #            .option(\"password\", mysql_properties[\"password\"])\n",
    "    #            .option(\"driver\", mysql_properties[\"driver\"])\n",
    "    #            .load())\n",
    "    # read_df.show(5)\n",
    "\n",
    "def staging_to_real():\n",
    "    \n",
    "    import mysql.connector\n",
    "\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"mysql_db_container\",\n",
    "        port=3306,\n",
    "        user=\"root\",\n",
    "        password=\"!QAZ2wsx\",\n",
    "        database=\"whole_corp\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    upsert_sql = f\"\"\"\n",
    "    INSERT INTO wholecorp_clusters_vector (統一編號, cluster, vector)\n",
    "    SELECT 統一編號, cluster, vector FROM staging_clusters_vector\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        cluster = VALUES(cluster),\n",
    "        vector  = VALUES(vector);\n",
    "    \"\"\"\n",
    "    cursor.execute(upsert_sql)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def fit_predict(data_whole, s, sample_frac=0.005, sample_cap=5000, batch_size=50000):\n",
    "    \n",
    "    try:\n",
    "        \"\"\"\n",
    "        Run SVD + MiniBatchKMeans on a manageable sample.\n",
    "        Returns: Spark DataFrame [統一編號, cluster, Xr_vector]\n",
    "        \"\"\"\n",
    "    \n",
    "        # --- get feature size from one row\n",
    "        first_vec = data_whole.limit(1).collect()[0][2]\n",
    "        num_features = int(first_vec.size)\n",
    "    \n",
    "        # --- helper: Spark Row -> CSR batch\n",
    "        def to_csr(rows):\n",
    "            indptr = [0]; indices = []; vals = []\n",
    "            for sv in rows:\n",
    "                indices.extend(sv.indices.tolist())\n",
    "                vals.extend(sv.values.tolist())\n",
    "                indptr.append(indptr[-1] + len(sv.indices))\n",
    "            return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                               np.array(indices, dtype=np.int32),\n",
    "                               np.array(indptr, dtype=np.int32)),\n",
    "                              shape=(len(rows), num_features))\n",
    "    \n",
    "        # 1) collect a small sample for SVD training\n",
    "        sample_rows = []\n",
    "        for i, row in enumerate(data_whole.sample(False, sample_frac, seed=42).toLocalIterator()):\n",
    "            sample_rows.append(row['features'])\n",
    "            if i >= sample_cap:   # hard cap\n",
    "                break\n",
    "    \n",
    "        X_sample = to_csr(sample_rows)\n",
    "    \n",
    "        svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "    \n",
    "        # # # --- Stage 2: incremental clustering\n",
    "        # kmeans = MiniBatchKMeans(\n",
    "        #     n_clusters=15,\n",
    "        #     random_state=42,\n",
    "        #     batch_size=batch_size,\n",
    "        #     verbose=1,\n",
    "        #     n_init='auto'\n",
    "        # )\n",
    "    \n",
    "        # batch_features, batch_ids = [], []\n",
    "        \n",
    "        # count = 0\n",
    "        # # --- Pass 1: training only\n",
    "        # for row in data_whole.select(\"統一編號\", \"features\").toLocalIterator():\n",
    "        #     batch_features.append(row[\"features\"])\n",
    "        #     batch_ids.append(row[\"統一編號\"])\n",
    "            \n",
    "        #     if len(batch_features) >= batch_size:\n",
    "        #         # print(f\"Training batch {count}\")\n",
    "        #         Xb = to_csr(batch_features)\n",
    "        #         Xr = svd.transform(Xb)\n",
    "        \n",
    "        #         kmeans.partial_fit(Xr)\n",
    "        \n",
    "        #         batch_features.clear(); batch_ids.clear()\n",
    "        #         count += batch_size\n",
    "        \n",
    "        # # train on leftovers\n",
    "        # if batch_features:\n",
    "        #     Xb = to_csr(batch_features)\n",
    "        #     Xr = svd.transform(Xb)\n",
    "        #     kmeans.partial_fit(Xr)\n",
    "    \n",
    "        # # Save kmeans model after training\n",
    "        # joblib.dump(kmeans, \"/tmp/kmeans_model.pkl\")\n",
    "        # print(\"KMeans model saved to /tmp/kmeans_model.pkl\")\n",
    "    \n",
    "        print(f'Loading kmean model')\n",
    "        kmeans = joblib.load(\"/tmp/kmeans_model.pkl\")\n",
    "        \n",
    "        # --- Pass 2: prediction with final centers\n",
    "        batch_features, batch_ids = [], []  # reset before 2nd loop\n",
    "        batch_count = 1\n",
    "        for row in data_whole.select(\"統一編號\", \"features\").toLocalIterator():\n",
    "            batch_features.append(row[\"features\"])\n",
    "            batch_ids.append(row[\"統一編號\"])\n",
    "        \n",
    "            if len(batch_features) >= batch_size:\n",
    "                # print(f\"Predicting batch {count}\")\n",
    "                Xb = to_csr(batch_features)\n",
    "                Xr = svd.transform(Xb)\n",
    "        \n",
    "                preds = kmeans.predict(Xr)\n",
    "        \n",
    "                ids = batch_ids\n",
    "                labels = preds.tolist()\n",
    "                vectors = Xr.tolist()\n",
    "    \n",
    "                print(f\"start inserting batch {batch_count}\")\n",
    "                to_mysql(ids, labels, vectors, s, batch_size)\n",
    "        \n",
    "                batch_features.clear(); batch_ids.clear()\n",
    "                batch_count += 1\n",
    "        \n",
    "        # predict leftovers\n",
    "        if batch_features:\n",
    "            Xb = to_csr(batch_features)\n",
    "            Xr = svd.transform(Xb)\n",
    "            preds = kmeans.predict(Xr)\n",
    "        \n",
    "            ids = batch_ids\n",
    "            labels = preds.tolist()\n",
    "            vectors = Xr.tolist()\n",
    "        \n",
    "            to_mysql(ids, labels, vectors, s, batch_size)\n",
    "    \n",
    "        print(f\"All batches are are inserted. Now moving data from staging to \")\n",
    "\n",
    "        staging_to_real()\n",
    "        \n",
    "        return svd, kmeans\n",
    "    except Exception as e:\n",
    "        logger.error(f'error occured {e}', exc_info=True)\n",
    "\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    pass\n",
    "    # s3 = boto3.client(\n",
    "    #     \"s3\",\n",
    "    #     endpoint_url = 'http://minio:9000',\n",
    "    #     aws_access_key_id=\"minioadmin\",\n",
    "    #     aws_secret_access_key=\"minioadmin\"\n",
    "    # )\n",
    "\n",
    "    # # save locally\n",
    "    # joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    # joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # # upload to MinIO\n",
    "    # s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    # s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")\n",
    "    \n",
    "# from sparksession import spark_session\n",
    "\n",
    "s = None\n",
    "s = spark_session()\n",
    "\n",
    "df = read_gold(s)\n",
    "\n",
    "svd, kmeans = fit_predict(df, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796e2b2-9152-4c58-8e07-0889c7a60bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr /= (np.linalg.norm(Xr, axis=1, keepdims=True) + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0267525-57cf-4fc6-8d03-79ba515702d9",
   "metadata": {},
   "source": [
    "# Metrics to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031865c-9eff-48cd-b74a-732dfe9e3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "for k in range(2,20):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(Xr)\n",
    "    km.fit(Xr)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    score = silhouette_score(Xr, km.labels_)\n",
    "    print(f\"k={k}, silhouette={score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4a1ab-a9ce-4a76-9f2c-2c24a9d69b04",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52026c69-ca61-4e7f-8890-8cb5ef4c36d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
