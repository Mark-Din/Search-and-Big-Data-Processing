{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9278d2b-7789-42e8-a1cd-a41e59a6d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b011ce0-f5c0-42a5-9a1f-97a4bfc1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_session():\n",
    "    # Stop any old session so new configs take effect in notebooks\n",
    "    return (\n",
    "        SparkSession.builder\n",
    "        .appName(\"MySQL_to_Delta_on_MinIO\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \",\".join([\n",
    "                    # Delta\n",
    "                    \"io.delta:delta-spark_2.12:3.1.0\",\n",
    "                    # MySQL JDBC\n",
    "                    \"mysql:mysql-connector-java:8.0.33\",\n",
    "                    # S3A / MinIO (versions must match your Hadoop)\n",
    "                    \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "                    \"com.amazonaws:aws-java-sdk-bundle:1.11.1026\",\n",
    "                ]))\n",
    "        # Delta integration\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        # MinIO (S3A) configs\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .config(\"spark.ui.port\", \"4040\")                 # fix the port\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")   # listen on all ifaces\n",
    "        .config(\"spark.driver.host\", \"jupyter\")          # OR \"spark-master\" – the container's DNS name\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "        # Resources\n",
    "        # .config(\"spark.executor.cores\", \"2\")\n",
    "        # .config(\"spark.executor.memory\", \"2g\")\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"1536m\")\n",
    "        # .config(\"spark.network.timeout\", \"600s\")\n",
    "        .config(\"spark.executor.cores\", \"2\")           # 1 task per executor (more stable for trees)\n",
    "        .config(\"spark.executor.memory\", \"3g\")\n",
    "        .config(\"spark.executor.memoryOverhead\", \"1g\")  # or omit in Standalone\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "        .config(\"spark.local.dir\", \"/mnt/spark-tmp/local\") # For giving it much more space to run CV\n",
    "        .config(\"spark.network.timeout\", \"600s\")\n",
    "        .getOrCreate()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d60bd71-e84f-47b2-979b-bf57564d0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.sparse import vstack as sp_vstack\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BinaryType, IntegerType, StructType, StructField, ArrayType, DoubleType\n",
    "\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_gold(spark):\n",
    "    global OUT\n",
    "\n",
    "    GOLD = os.getenv(\"GOLD_PATH\",\"s3a://deltabucket/gold/wholeCorp_delta\")\n",
    "    OUT = os.getenv(\"CLUSTER_PATH\",\"s3a://deltabucket/gold/wholeCorp_clusters\")\n",
    "\n",
    "    return spark.read.format(\"delta\").load(GOLD)\n",
    "\n",
    "\n",
    "def fit_predict(data_whole):\n",
    "    # --- get feature size from one row\n",
    "    first_vec = data_whole.limit(1).collect()[0][2]\n",
    "    num_features = int(first_vec.size)\n",
    "\n",
    "    # --- helper: Spark Row -> CSR batch\n",
    "    def to_csr(rows):\n",
    "        indptr = [0]; indices = []; vals = []\n",
    "        for sv in rows:\n",
    "            indices.extend(sv.indices.tolist())\n",
    "            vals.extend(sv.values.tolist())\n",
    "            indptr.append(indptr[-1] + len(sv.indices))\n",
    "        return csr_matrix((np.array(vals, dtype=np.float64),\n",
    "                        np.array(indices, dtype=np.int32),\n",
    "                        np.array(indptr, dtype=np.int32)),\n",
    "                        shape=(len(rows), num_features\n",
    "    ))\n",
    "\n",
    "    # 1) collect a manageable sample from Spark\n",
    "    sample_rows = []\n",
    "    for i, row in enumerate(data_whole.sample(False, 0.02, seed=42).toLocalIterator()):  # ~2% example\n",
    "        sample_rows.append(row['features'])\n",
    "        if i >= 20000:      # cap by count if you like\n",
    "            break\n",
    "\n",
    "    X_sample = to_csr(sample_rows)            # CSR (n_sample, num_features)\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42).fit(X_sample)\n",
    "\n",
    "    # --- Stage 2: fit MiniBatchKMeans on reduced features\n",
    "    kmeans = MiniBatchKMeans(n_clusters=15,\n",
    "                            random_state=42,\n",
    "                            batch_size=2000,\n",
    "                            verbose=1,\n",
    "                            n_init='auto')\n",
    "    ids, labels = [], []\n",
    "    batch_features, batch_ids = [], []\n",
    "\n",
    "    for row in data_whole.select(\"統一編號\",\"features\").toLocalIterator():\n",
    "        batch_features.append(row[\"features\"])\n",
    "        batch_ids.append(row[\"統一編號\"])\n",
    "        if len(batch_features) >= 2000:\n",
    "            Xb = to_csr(batch_features)\n",
    "            Xr = svd.transform(Xb)\n",
    "            kmeans.partial_fit(Xr)\n",
    "\n",
    "            preds = kmeans.predict(Xr)            # ndarray\n",
    "            ids.extend(batch_ids)                  # flatten ids\n",
    "            labels.extend(preds.tolist())          # flatten labels\n",
    "            batch_features.clear(); batch_ids.clear()\n",
    "\n",
    "    if batch_features:\n",
    "        Xb = to_csr(batch_features)\n",
    "        Xr = svd.transform(Xb)\n",
    "        kmeans.partial_fit(Xr)\n",
    "        preds = kmeans.predict(Xr)\n",
    "        ids.extend(batch_ids)\n",
    "        labels.extend(preds.tolist())\n",
    "\n",
    "    pdf = pd.DataFrame({\"統一編號\": ids, \"cluster\": labels})\n",
    "\n",
    "    return pdf, svd, kmeans\n",
    "\n",
    "\n",
    "def save_(pdf, spark, kmeans, svd):\n",
    "\n",
    "    # spark.createDataFrame(pdf).write.format(\"delta\").mode(\"overwrite\").save(OUT)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url = 'http://minio:9000',\n",
    "        aws_access_key_id=\"minioadmin\",\n",
    "        aws_secret_access_key=\"minioadmin\"\n",
    "    )\n",
    "\n",
    "    # save locally\n",
    "    joblib.dump(svd, \"/tmp/svd.pkl\")\n",
    "    joblib.dump(kmeans, \"/tmp/kmeans.pkl\")\n",
    "    \n",
    "    # upload to MinIO\n",
    "    s3.upload_file(\"/tmp/svd.pkl\", \"deltabucket\", \"models/sk_svd.pkl\")\n",
    "    s3.upload_file(\"/tmp/kmeans.pkl\", \"deltabucket\", \"models/sk_kmeans.pkl\")\n",
    "\n",
    "def main():\n",
    "    # from sparksession import spark_session\n",
    "    s = None\n",
    "    try:\n",
    "        s = spark_session()\n",
    "        df = read_gold(s)\n",
    "        pdf, svd, kmeans = fit_predict(df)\n",
    "        save_(pdf, s, kmeans, svd)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spark job: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908307a6-ea0e-490c-85fa-8ddf836999b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MiniBatchKMeans] Reassigning 10 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 2 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd50ed7-bdab-4a02-a2e1-3bf563bc18e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf93c22-d6f7-410f-ad6a-1223ca6e13d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679fa6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\recommend_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 系統找不到指定的檔案。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\miniconda3\\envs\\recommend_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"d:\\miniconda3\\envs\\recommend_env\\lib\\subprocess.py\", line 493, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"d:\\miniconda3\\envs\\recommend_env\\lib\\subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\miniconda3\\envs\\recommend_env\\lib\\subprocess.py\", line 1327, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.43986817  0.55876849]\n",
      " [ 9.26648681  0.47140452]\n",
      " [ 2.53859104  7.5748854 ]\n",
      " [ 1.20185043  9.45404088]\n",
      " [10.5200338   0.88254682]\n",
      " [ 2.60341656 12.33878258]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\recommend_env\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1440: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
    "\n",
    "# Initialize and fit KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto')\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = kmeans.transform(X)\n",
    "\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b0a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0267525-57cf-4fc6-8d03-79ba515702d9",
   "metadata": {},
   "source": [
    "# Metrics to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031865c-9eff-48cd-b74a-732dfe9e3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "wcss = []\n",
    "for k in range(2,20):\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init='auto').fit(Xr)\n",
    "    km.fit(Xr)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "    score = silhouette_score(Xr, km.labels_)\n",
    "    print(f\"k={k}, silhouette={score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4a1ab-a9ce-4a76-9f2c-2c24a9d69b04",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
